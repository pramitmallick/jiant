12/16 02:14:09 AM: fastText library not found!
12/16 02:14:10 AM: Loading config from config/demo.conf
12/16 02:14:10 AM: Config overrides: exp_name = wmt, run_name = foobar, d_hid = 256, load_model = 1
12/16 02:14:10 AM: Waiting on git info....
12/16 02:14:10 AM: Git branch: master
12/16 02:14:10 AM: Git SHA: 3594dae32ed113f9ffb7599bd5d583d1f307b405
12/16 02:14:10 AM: Parsed args: 
{
  "allow_missing_task_map": 0,
  "allow_reuse_of_pretraining_parameters": 0,
  "allow_untrained_encoder_parameters": 0,
  "batch_size": 8,
  "bidirectional": 1,
  "bpp_base": 1,
  "char_embs": 0,
  "char_filter_sizes": "2,3,4,5",
  "classifier": "mlp",
  "classifier_dropout": 0.2,
  "classifier_hid_dim": 32,
  "classifier_loss_fn": "",
  "classifier_span_pooling": "x,y",
  "cola": {},
  "cola_classifier_dropout": 0.2,
  "cola_classifier_hid_dim": 256,
  "cola_d_proj": 256,
  "cola_lr": 0.0003,
  "cola_val_interval": 100,
  "cove": 0,
  "cove_fine_tune": 0,
  "cuda": 0,
  "d_char": 100,
  "d_ff": 2048,
  "d_hid": 256,
  "d_hid_attn": 512,
  "d_proj": 512,
  "d_tproj": 64,
  "d_word": 300,
  "data_dir": "data",
  "dec_val_scale": 250,
  "do_full_eval": 1,
  "do_pretrain": 1,
  "do_target_task_training": 1,
  "dropout": 0.2,
  "dropout_embs": 0.2,
  "edgeprobe_cnn_context": 0,
  "edges-ccg-parse": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-ccg-tag": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-constituent-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-constituent-ontonotes-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-constituent-ptb": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-coref-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-coref-ontonotes-conll": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-coref-ontonotes-conll-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-dep-labeling": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-dep-labeling-ewt": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-dep-labeling-ewt-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-dpr": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-dpr-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-ner-conll2003": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 250
  },
  "edges-ner-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-ner-ontonotes-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-nonterminal-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-pos-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-spr1": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-spr1-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-spr2": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-spr2-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-srl-conll2005": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-srl-conll2012": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-srl-conll2012-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-tmpl": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "elmo": 1,
  "elmo_chars_only": 1,
  "elmo_weight_file_path": "none",
  "eval_data_fraction": 1,
  "eval_max_vals": 10,
  "eval_val_interval": 10,
  "exp_dir": "outputs/wmt/",
  "exp_name": "wmt",
  "fastText": 0,
  "fastText_model_file": ".",
  "global_ro_exp_dir": "/nfs/jsalt/share/exp/demo",
  "grounded": {},
  "grounded_d_proj": 2048,
  "groundedsw": {},
  "groundedsw_d_proj": 2048,
  "is_probing_task": 0,
  "keep_all_checkpoints": 0,
  "load_eval_checkpoint": "none",
  "load_model": 1,
  "local_log_path": "outputs/wmt/foobar/log.log",
  "lr": 0.0001,
  "lr_decay_factor": 0.5,
  "lr_patience": 1,
  "max_char_v_size": 250,
  "max_grad_norm": 5.0,
  "max_seq_len": 10,
  "max_targ_word_v_size": 20000,
  "max_vals": 10,
  "max_word_v_size": 5000,
  "min_lr": 1e-06,
  "mnli": {},
  "mnli-alt": {},
  "mnli-alt_classifier_dropout": 0.2,
  "mnli-alt_classifier_hid_dim": 512,
  "mnli-alt_lr": 0.0003,
  "mnli-alt_pair_attn": 1,
  "mnli-alt_val_interval": 1000,
  "mnli-diagnostic": {
    "use_classifier": "mnli"
  },
  "mnli_classifier_dropout": 0.2,
  "mnli_classifier_hid_dim": 512,
  "mnli_lr": 0.0003,
  "mnli_pair_attn": 1,
  "mnli_val_interval": 1000,
  "mrpc": {
    "classifier_dropout": 0.1,
    "classifier_hid_dim": 256,
    "max_vals": 8,
    "val_interval": 1
  },
  "mrpc_classifier_dropout": 0.2,
  "mrpc_classifier_hid_dim": 256,
  "mrpc_d_proj": 256,
  "mrpc_lr": 0.0003,
  "mrpc_pair_attn": 0,
  "mrpc_val_interval": 100,
  "n_char_filters": 100,
  "n_heads": 8,
  "n_layers_enc": 1,
  "n_layers_highway": 0,
  "nli-prob": {
    "probe_path": ""
  },
  "openai_embeddings_mode": "none",
  "openai_transformer": 0,
  "openai_transformer_ckpt": "",
  "openai_transformer_fine_tune": 0,
  "optimizer": "adam",
  "pair_attn": 1,
  "patience": 5,
  "pretrain_tasks": "sst,mrpc,wnli",
  "project_dir": "outputs",
  "qnli": {},
  "qnli-alt": {},
  "qnli-alt_classifier_dropout": 0.2,
  "qnli-alt_classifier_hid_dim": 512,
  "qnli-alt_lr": 0.0003,
  "qnli-alt_pair_attn": 1,
  "qnli-alt_val_interval": 1000,
  "qnli_classifier_dropout": 0.2,
  "qnli_classifier_hid_dim": 512,
  "qnli_lr": 0.0003,
  "qnli_pair_attn": 1,
  "qnli_val_interval": 1000,
  "qqp": {},
  "qqp-alt": {},
  "qqp-alt_classifier_dropout": 0.2,
  "qqp-alt_classifier_hid_dim": 512,
  "qqp-alt_lr": 0.0003,
  "qqp-alt_pair_attn": 1,
  "qqp-alt_val_interval": 1000,
  "qqp_classifier_dropout": 0.2,
  "qqp_classifier_hid_dim": 512,
  "qqp_lr": 0.0003,
  "qqp_pair_attn": 1,
  "qqp_val_interval": 1000,
  "random_seed": 42,
  "reindex_tasks": "",
  "reload_indexing": 0,
  "reload_tasks": 0,
  "reload_vocab": 0,
  "remote_log_name": "wmt__foobar",
  "rte": {},
  "rte_classifier_dropout": 0.4,
  "rte_classifier_hid_dim": 128,
  "rte_d_proj": 128,
  "rte_lr": 0.0003,
  "rte_pair_attn": 0,
  "rte_val_interval": 100,
  "run_dir": "outputs/wmt/foobar",
  "run_name": "foobar",
  "s2s": {
    "attention": "bilinear",
    "d_hid_dec": 1024,
    "n_layers_dec": 1,
    "output_proj_input_dim": 1024,
    "target_embedding_dim": 300
  },
  "scaling_method": "uniform",
  "scheduler_threshold": 0.0001,
  "sent_enc": "rnn",
  "sep_embs_for_skip": 0,
  "shared_optimizer": 1,
  "shared_pair_attn": 0,
  "skip_embs": 1,
  "sst": {},
  "sst_classifier_dropout": 0.2,
  "sst_classifier_hid_dim": 256,
  "sst_d_proj": 256,
  "sst_lr": 0.0003,
  "sst_val_interval": 100,
  "sts-b": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 512,
    "max_vals": 16,
    "pair_attn": 0,
    "val_interval": 100
  },
  "sts-b-alt": {},
  "sts-b-alt_classifier_dropout": 0.2,
  "sts-b-alt_classifier_hid_dim": 512,
  "sts-b-alt_lr": 0.0003,
  "sts-b-alt_pair_attn": 1,
  "sts-b-alt_val_interval": 1000,
  "sts-b_classifier_dropout": 0.2,
  "sts-b_classifier_hid_dim": 512,
  "sts-b_lr": 0.0003,
  "sts-b_pair_attn": 1,
  "sts-b_val_interval": 1000,
  "target_tasks": "wmt14_en_de",
  "track_batch_utilization": 0,
  "trainer_type": "sampling",
  "training_data_fraction": 1,
  "use_classifier": "",
  "val_data_limit": 5000,
  "val_interval": 100,
  "warmup": 4000,
  "weighting_method": "uniform",
  "wnli": {},
  "wnli_classifier_dropout": 0.4,
  "wnli_classifier_hid_dim": 128,
  "wnli_d_proj": 128,
  "wnli_lr": 0.0003,
  "wnli_pair_attn": 0,
  "wnli_val_interval": 100,
  "word_embs": "none",
  "word_embs_file": "embeddings/glove.840B.300d.txt",
  "write_preds": 0,
  "write_strict_glue_format": 0
}
12/16 02:14:10 AM: Saved config to outputs/wmt/foobar/params.conf
12/16 02:14:10 AM: Using random seed 42
12/16 02:14:10 AM: Using GPU 0
12/16 02:14:10 AM: Loading tasks...
12/16 02:14:10 AM: Writing pre-preprocessed tasks to outputs/wmt/
12/16 02:14:10 AM: 	Loaded existing task mrpc
12/16 02:14:10 AM: 	Task 'mrpc': train=3668 val=408 test=1725
12/16 02:14:10 AM: 	Loaded existing task sst
12/16 02:14:10 AM: 	Task 'sst': train=67349 val=872 test=1821
12/16 02:14:10 AM: 	Loaded existing task wmt14_en_de
12/16 02:15:03 AM: 	Task 'wmt14_en_de': train=3434778 val=34681 test=3434778
12/16 02:15:03 AM: 	Loaded existing task wnli
12/16 02:15:03 AM: 	Task 'wnli': train=635 val=71 test=146
12/16 02:15:03 AM: 	Finished loading tasks: mrpc sst wmt14_en_de wnli.
12/16 02:15:03 AM: Loading token dictionary from outputs/wmt/vocab.
12/16 02:15:03 AM: 	Loaded vocab from outputs/wmt/vocab
12/16 02:15:03 AM: 	Vocab namespace chars: size 252
12/16 02:15:03 AM: 	Vocab namespace wmt14_en_de_tokens: size 20002
12/16 02:15:03 AM: 	Vocab namespace tokens: size 5002
12/16 02:15:03 AM: 	Finished building vocab.
12/16 02:15:03 AM: 	Task 'mrpc', split 'train': Found preprocessed copy in outputs/wmt/preproc/mrpc__train_data
12/16 02:15:03 AM: 	Task 'mrpc', split 'val': Found preprocessed copy in outputs/wmt/preproc/mrpc__val_data
12/16 02:15:03 AM: 	Task 'mrpc', split 'test': Found preprocessed copy in outputs/wmt/preproc/mrpc__test_data
12/16 02:15:03 AM: 	Task 'mrpc': cleared in-memory data.
12/16 02:15:03 AM: 	Task 'sst', split 'train': Found preprocessed copy in outputs/wmt/preproc/sst__train_data
12/16 02:15:03 AM: 	Task 'sst', split 'val': Found preprocessed copy in outputs/wmt/preproc/sst__val_data
12/16 02:15:03 AM: 	Task 'sst', split 'test': Found preprocessed copy in outputs/wmt/preproc/sst__test_data
12/16 02:15:03 AM: 	Task 'sst': cleared in-memory data.
12/16 02:15:03 AM: 	Task 'wmt14_en_de', split 'train': Found preprocessed copy in outputs/wmt/preproc/wmt14_en_de__train_data
12/16 02:15:03 AM: 	Task 'wmt14_en_de', split 'val': Found preprocessed copy in outputs/wmt/preproc/wmt14_en_de__val_data
12/16 02:15:03 AM: 	Task 'wmt14_en_de', split 'test': Found preprocessed copy in outputs/wmt/preproc/wmt14_en_de__test_data
12/16 02:15:03 AM: 	Task 'wmt14_en_de': cleared in-memory data.
12/16 02:15:03 AM: 	Task 'wnli', split 'train': Found preprocessed copy in outputs/wmt/preproc/wnli__train_data
12/16 02:15:03 AM: 	Task 'wnli', split 'val': Found preprocessed copy in outputs/wmt/preproc/wnli__val_data
12/16 02:15:03 AM: 	Task 'wnli', split 'test': Found preprocessed copy in outputs/wmt/preproc/wnli__test_data
12/16 02:15:03 AM: 	Task 'wnli': cleared in-memory data.
12/16 02:15:03 AM: 	Finished indexing tasks
12/16 02:15:03 AM: 	Lazy-loading indexed data for task='mrpc' from outputs/wmt/preproc
12/16 02:15:03 AM: 	Lazy-loading indexed data for task='sst' from outputs/wmt/preproc
12/16 02:15:03 AM: 	Lazy-loading indexed data for task='wmt14_en_de' from outputs/wmt/preproc
12/16 02:15:03 AM: 	Lazy-loading indexed data for task='wnli' from outputs/wmt/preproc
12/16 02:15:03 AM: All tasks initialized with data iterators.
12/16 02:15:03 AM: 	  Training on sst, mrpc, wnli
12/16 02:15:03 AM: 	  Evaluating on wmt14_en_de
12/16 02:15:03 AM: 	Finished loading tasks in 52.638s
12/16 02:15:03 AM: 	 Tasks: ['mrpc', 'sst', 'wmt14_en_de', 'wnli']
12/16 02:15:03 AM: Building model...
12/16 02:15:03 AM: 	Not using word embeddings!
12/16 02:15:03 AM: 	Not using character embeddings!
12/16 02:15:03 AM: Loading ELMo from files:
12/16 02:15:03 AM: ELMO_OPT_PATH = https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json
12/16 02:15:03 AM: 	Using ELMo character CNN only!
12/16 02:15:03 AM: ELMO_WEIGHTS_PATH = https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5
12/16 02:15:08 AM: batch_first = True
12/16 02:15:08 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:08 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:08 AM: input_size = 512
12/16 02:15:08 AM: bidirectional = True
12/16 02:15:08 AM: hidden_size = 256
12/16 02:15:08 AM: num_layers = 1
12/16 02:15:08 AM: batch_first = True
12/16 02:15:08 AM: Initializing parameters
12/16 02:15:08 AM: Done initializing parameters; the following parameters are using their default initialization from their code
12/16 02:15:08 AM:    _phrase_layer._module.bias_hh_l0
12/16 02:15:08 AM:    _phrase_layer._module.bias_hh_l0_reverse
12/16 02:15:08 AM:    _phrase_layer._module.bias_ih_l0
12/16 02:15:08 AM:    _phrase_layer._module.bias_ih_l0_reverse
12/16 02:15:08 AM:    _phrase_layer._module.weight_hh_l0
12/16 02:15:08 AM:    _phrase_layer._module.weight_hh_l0_reverse
12/16 02:15:08 AM:    _phrase_layer._module.weight_ih_l0
12/16 02:15:08 AM:    _phrase_layer._module.weight_ih_l0_reverse
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo._char_embedding_weights
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo._highways._layers.0.bias
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo._highways._layers.0.weight
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo._highways._layers.1.bias
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo._highways._layers.1.weight
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo._projection.bias
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo._projection.weight
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_0.bias
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_0.weight
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_1.bias
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_1.weight
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_2.bias
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_2.weight
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_3.bias
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_3.weight
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_4.bias
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_4.weight
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_5.bias
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_5.weight
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_6.bias
12/16 02:15:08 AM:    _text_field_embedder.token_embedder_elmo.char_conv_6.weight
12/16 02:15:08 AM: Using BiLSTM architecture for shared encoder!
12/16 02:15:08 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:08 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:08 AM: cls_type = mlp
12/16 02:15:08 AM: d_hid = 128
12/16 02:15:08 AM: d_proj = 128
12/16 02:15:08 AM: shared_pair_attn = 0
12/16 02:15:08 AM: attn = 0
12/16 02:15:08 AM: d_hid_attn = 512
12/16 02:15:08 AM: dropout = 0.4
12/16 02:15:08 AM: cls_loss_fn = 
12/16 02:15:08 AM: cls_span_pooling = x,y
12/16 02:15:08 AM: edgeprobe_cnn_context = 0
12/16 02:15:08 AM: use_classifier = wnli
12/16 02:15:08 AM: 	Task 'wnli' params: {
  "cls_type": "mlp",
  "d_hid": 128,
  "d_proj": 128,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.4,
  "cls_loss_fn": "",
  "cls_span_pooling": "x,y",
  "edgeprobe_cnn_context": 0,
  "use_classifier": "wnli"
}
12/16 02:15:08 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:08 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:08 AM: cls_type = mlp
12/16 02:15:08 AM: d_hid = 256
12/16 02:15:08 AM: d_proj = 256
12/16 02:15:08 AM: shared_pair_attn = 0
12/16 02:15:08 AM: attn = 0
12/16 02:15:08 AM: d_hid_attn = 512
12/16 02:15:08 AM: dropout = 0.1
12/16 02:15:08 AM: cls_loss_fn = 
12/16 02:15:08 AM: cls_span_pooling = x,y
12/16 02:15:08 AM: edgeprobe_cnn_context = 0
12/16 02:15:08 AM: use_classifier = mrpc
12/16 02:15:08 AM: 	Task 'mrpc' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "d_proj": 256,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.1,
  "cls_loss_fn": "",
  "cls_span_pooling": "x,y",
  "edgeprobe_cnn_context": 0,
  "use_classifier": "mrpc"
}
12/16 02:15:08 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:08 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:08 AM: cls_type = mlp
12/16 02:15:08 AM: d_hid = 32
12/16 02:15:08 AM: d_proj = 512
12/16 02:15:08 AM: shared_pair_attn = 0
12/16 02:15:08 AM: attn = 1
12/16 02:15:08 AM: d_hid_attn = 512
12/16 02:15:08 AM: dropout = 0.2
12/16 02:15:08 AM: cls_loss_fn = 
12/16 02:15:08 AM: cls_span_pooling = x,y
12/16 02:15:08 AM: edgeprobe_cnn_context = 0
12/16 02:15:08 AM: use_classifier = wmt14_en_de
12/16 02:15:08 AM: 	Task 'wmt14_en_de' params: {
  "cls_type": "mlp",
  "d_hid": 32,
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "",
  "cls_span_pooling": "x,y",
  "edgeprobe_cnn_context": 0,
  "use_classifier": "wmt14_en_de"
}
12/16 02:15:08 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:08 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:08 AM: cls_type = mlp
12/16 02:15:08 AM: d_hid = 256
12/16 02:15:08 AM: d_proj = 256
12/16 02:15:08 AM: shared_pair_attn = 0
12/16 02:15:08 AM: attn = 1
12/16 02:15:08 AM: d_hid_attn = 512
12/16 02:15:08 AM: dropout = 0.2
12/16 02:15:08 AM: cls_loss_fn = 
12/16 02:15:08 AM: cls_span_pooling = x,y
12/16 02:15:08 AM: edgeprobe_cnn_context = 0
12/16 02:15:08 AM: use_classifier = sst
12/16 02:15:08 AM: 	Task 'sst' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "d_proj": 256,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "",
  "cls_span_pooling": "x,y",
  "edgeprobe_cnn_context": 0,
  "use_classifier": "sst"
}
12/16 02:15:08 AM: using bilinear attention
12/16 02:15:11 AM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): ElmoTextFieldEmbedder(
      (token_embedder_elmo): ElmoCharacterEncoder(
        (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
        (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))
        (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))
        (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))
        (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))
        (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))
        (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))
        (_highways): Highway(
          (_layers): ModuleList(
            (0): Linear(in_features=2048, out_features=4096, bias=True)
            (1): Linear(in_features=2048, out_features=4096, bias=True)
          )
        )
        (_projection): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): PytorchSeq2SeqWrapper(
      (_module): LSTM(512, 256, batch_first=True, bidirectional=True)
    )
    (_dropout): Dropout(p=0.2)
  )
  (mrpc_mdl): PairClassifier(
    (pooler): Pooler(
      (project): Linear(in_features=1024, out_features=256, bias=True)
    )
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.1)
        (4): Linear(in_features=256, out_features=2, bias=True)
      )
    )
  )
  (sst_mdl): SingleClassifier(
    (pooler): Pooler(
      (project): Linear(in_features=1024, out_features=256, bias=True)
    )
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.2)
        (4): Linear(in_features=256, out_features=2, bias=True)
      )
    )
  )
  (wmt14_en_de_decoder): Seq2SeqDecoder(
    (_target_embedder): Embedding()
    (_sent_pooler): Pooler(
      (project): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (_decoder_attention): BilinearAttention()
    (_decoder_cell): LSTMCell(1324, 1024)
    (_output_projection_layer): Linear(in_features=1024, out_features=20002, bias=True)
    (_dropout): Dropout(p=0.2)
  )
  (wnli_mdl): PairClassifier(
    (pooler): Pooler(
      (project): Linear(in_features=1024, out_features=128, bias=True)
    )
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=512, out_features=128, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.4)
        (4): Linear(in_features=128, out_features=2, bias=True)
      )
    )
  )
)
12/16 02:15:11 AM: >> Trainable param sent_encoder._phrase_layer._module.weight_ih_l0: torch.Size([1024, 512]) = 524288
12/16 02:15:11 AM: >> Trainable param sent_encoder._phrase_layer._module.weight_hh_l0: torch.Size([1024, 256]) = 262144
12/16 02:15:11 AM: >> Trainable param sent_encoder._phrase_layer._module.bias_ih_l0: torch.Size([1024]) = 1024
12/16 02:15:11 AM: >> Trainable param sent_encoder._phrase_layer._module.bias_hh_l0: torch.Size([1024]) = 1024
12/16 02:15:11 AM: >> Trainable param sent_encoder._phrase_layer._module.weight_ih_l0_reverse: torch.Size([1024, 512]) = 524288
12/16 02:15:11 AM: >> Trainable param sent_encoder._phrase_layer._module.weight_hh_l0_reverse: torch.Size([1024, 256]) = 262144
12/16 02:15:11 AM: >> Trainable param sent_encoder._phrase_layer._module.bias_ih_l0_reverse: torch.Size([1024]) = 1024
12/16 02:15:11 AM: >> Trainable param sent_encoder._phrase_layer._module.bias_hh_l0_reverse: torch.Size([1024]) = 1024
12/16 02:15:11 AM: >> Trainable param mrpc_mdl.pooler.project.weight: torch.Size([256, 1024]) = 262144
12/16 02:15:11 AM: >> Trainable param mrpc_mdl.pooler.project.bias: torch.Size([256]) = 256
12/16 02:15:11 AM: >> Trainable param mrpc_mdl.classifier.classifier.0.weight: torch.Size([256, 1024]) = 262144
12/16 02:15:11 AM: >> Trainable param mrpc_mdl.classifier.classifier.0.bias: torch.Size([256]) = 256
12/16 02:15:11 AM: >> Trainable param mrpc_mdl.classifier.classifier.2.weight: torch.Size([256]) = 256
12/16 02:15:11 AM: >> Trainable param mrpc_mdl.classifier.classifier.2.bias: torch.Size([256]) = 256
12/16 02:15:11 AM: >> Trainable param mrpc_mdl.classifier.classifier.4.weight: torch.Size([2, 256]) = 512
12/16 02:15:11 AM: >> Trainable param mrpc_mdl.classifier.classifier.4.bias: torch.Size([2]) = 2
12/16 02:15:11 AM: >> Trainable param sst_mdl.pooler.project.weight: torch.Size([256, 1024]) = 262144
12/16 02:15:11 AM: >> Trainable param sst_mdl.pooler.project.bias: torch.Size([256]) = 256
12/16 02:15:11 AM: >> Trainable param sst_mdl.classifier.classifier.0.weight: torch.Size([256, 256]) = 65536
12/16 02:15:11 AM: >> Trainable param sst_mdl.classifier.classifier.0.bias: torch.Size([256]) = 256
12/16 02:15:11 AM: >> Trainable param sst_mdl.classifier.classifier.2.weight: torch.Size([256]) = 256
12/16 02:15:11 AM: >> Trainable param sst_mdl.classifier.classifier.2.bias: torch.Size([256]) = 256
12/16 02:15:11 AM: >> Trainable param sst_mdl.classifier.classifier.4.weight: torch.Size([2, 256]) = 512
12/16 02:15:11 AM: >> Trainable param sst_mdl.classifier.classifier.4.bias: torch.Size([2]) = 2
12/16 02:15:11 AM: >> Trainable param wmt14_en_de_decoder._target_embedder.weight: torch.Size([20002, 300]) = 6000600
12/16 02:15:11 AM: >> Trainable param wmt14_en_de_decoder._sent_pooler.project.weight: torch.Size([1024, 1024]) = 1048576
12/16 02:15:11 AM: >> Trainable param wmt14_en_de_decoder._sent_pooler.project.bias: torch.Size([1024]) = 1024
12/16 02:15:11 AM: >> Trainable param wmt14_en_de_decoder._decoder_attention._weight_matrix: torch.Size([1024, 1024]) = 1048576
12/16 02:15:11 AM: >> Trainable param wmt14_en_de_decoder._decoder_attention._bias: torch.Size([1]) = 1
12/16 02:15:11 AM: >> Trainable param wmt14_en_de_decoder._decoder_cell.weight_ih: torch.Size([4096, 1324]) = 5423104
12/16 02:15:11 AM: >> Trainable param wmt14_en_de_decoder._decoder_cell.weight_hh: torch.Size([4096, 1024]) = 4194304
12/16 02:15:11 AM: >> Trainable param wmt14_en_de_decoder._decoder_cell.bias_ih: torch.Size([4096]) = 4096
12/16 02:15:11 AM: >> Trainable param wmt14_en_de_decoder._decoder_cell.bias_hh: torch.Size([4096]) = 4096
12/16 02:15:11 AM: >> Trainable param wmt14_en_de_decoder._output_projection_layer.weight: torch.Size([20002, 1024]) = 20482048
12/16 02:15:11 AM: >> Trainable param wmt14_en_de_decoder._output_projection_layer.bias: torch.Size([20002]) = 20002
12/16 02:15:11 AM: >> Trainable param wnli_mdl.pooler.project.weight: torch.Size([128, 1024]) = 131072
12/16 02:15:11 AM: >> Trainable param wnli_mdl.pooler.project.bias: torch.Size([128]) = 128
12/16 02:15:11 AM: >> Trainable param wnli_mdl.classifier.classifier.0.weight: torch.Size([128, 512]) = 65536
12/16 02:15:11 AM: >> Trainable param wnli_mdl.classifier.classifier.0.bias: torch.Size([128]) = 128
12/16 02:15:11 AM: >> Trainable param wnli_mdl.classifier.classifier.2.weight: torch.Size([128]) = 128
12/16 02:15:11 AM: >> Trainable param wnli_mdl.classifier.classifier.2.bias: torch.Size([128]) = 128
12/16 02:15:11 AM: >> Trainable param wnli_mdl.classifier.classifier.4.weight: torch.Size([2, 128]) = 256
12/16 02:15:11 AM: >> Trainable param wnli_mdl.classifier.classifier.4.bias: torch.Size([2]) = 2
12/16 02:15:11 AM: Total number of parameters: 58893665 (5.88937e+07)
12/16 02:15:11 AM: Number of trainable parameters: 40855809 (4.08558e+07)
12/16 02:15:11 AM: 	Finished building model in 8.607s
12/16 02:15:11 AM: Will run the following steps:
Training model on tasks: sst,mrpc,wnli
Re-training model for individual eval tasks
Evaluating model on tasks: wmt14_en_de
12/16 02:15:11 AM: Training...
12/16 02:15:11 AM: 	Using ReduceLROnPlateau scheduler!
12/16 02:15:11 AM: patience = 5
12/16 02:15:11 AM: val_interval = 100
12/16 02:15:11 AM: max_vals = 10
12/16 02:15:11 AM: cuda_device = 0
12/16 02:15:11 AM: grad_norm = 5.0
12/16 02:15:11 AM: grad_clipping = None
12/16 02:15:11 AM: lr_decay = 0.99
12/16 02:15:11 AM: min_lr = 1e-06
12/16 02:15:11 AM: keep_all_checkpoints = 0
12/16 02:15:11 AM: val_data_limit = 5000
12/16 02:15:11 AM: dec_val_scale = 250
12/16 02:15:11 AM: training_data_fraction = 1
12/16 02:15:11 AM: type = adam
12/16 02:15:11 AM: parameter_groups = None
12/16 02:15:11 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:11 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:11 AM: lr = 0.0001
12/16 02:15:11 AM: weight_decay = 0
12/16 02:15:11 AM: amsgrad = True
12/16 02:15:11 AM: type = reduce_on_plateau
12/16 02:15:11 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:11 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:11 AM: mode = max
12/16 02:15:11 AM: factor = 0.5
12/16 02:15:11 AM: patience = 1
12/16 02:15:11 AM: threshold = 0.0001
12/16 02:15:11 AM: threshold_mode = abs
12/16 02:15:11 AM: verbose = True
12/16 02:15:11 AM: type = adam
12/16 02:15:11 AM: parameter_groups = None
12/16 02:15:11 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:11 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:11 AM: lr = 0.0001
12/16 02:15:11 AM: weight_decay = 0
12/16 02:15:11 AM: amsgrad = True
12/16 02:15:11 AM: type = reduce_on_plateau
12/16 02:15:11 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:11 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:11 AM: mode = max
12/16 02:15:11 AM: factor = 0.5
12/16 02:15:11 AM: patience = 1
12/16 02:15:11 AM: threshold = 0.0001
12/16 02:15:11 AM: threshold_mode = abs
12/16 02:15:11 AM: verbose = True
12/16 02:15:11 AM: type = adam
12/16 02:15:11 AM: parameter_groups = None
12/16 02:15:11 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:11 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:11 AM: lr = 0.0001
12/16 02:15:11 AM: weight_decay = 0
12/16 02:15:11 AM: amsgrad = True
12/16 02:15:11 AM: type = reduce_on_plateau
12/16 02:15:11 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:11 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:11 AM: mode = max
12/16 02:15:11 AM: factor = 0.5
12/16 02:15:11 AM: patience = 1
12/16 02:15:11 AM: threshold = 0.0001
12/16 02:15:11 AM: threshold_mode = abs
12/16 02:15:11 AM: verbose = True
12/16 02:15:11 AM: type = adam
12/16 02:15:11 AM: parameter_groups = None
12/16 02:15:11 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:11 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:11 AM: lr = 0.0001
12/16 02:15:11 AM: weight_decay = 0
12/16 02:15:11 AM: amsgrad = True
12/16 02:15:11 AM: type = reduce_on_plateau
12/16 02:15:11 AM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/16 02:15:11 AM: CURRENTLY DEFINED PARAMETERS: 
12/16 02:15:11 AM: mode = max
12/16 02:15:11 AM: factor = 0.5
12/16 02:15:11 AM: patience = 1
12/16 02:15:11 AM: threshold = 0.0001
12/16 02:15:11 AM: threshold_mode = abs
12/16 02:15:11 AM: verbose = True
12/16 02:15:11 AM: Found checkpoint main_epoch_8.th. Loading.
12/16 02:15:14 AM: Loaded model from checkpoint. Starting at pass 800.
12/16 02:15:14 AM: Training examples per task: {'mrpc': 3668, 'sst': 67349, 'wnli': 635}
12/16 02:15:14 AM: Sampling tasks uniformly.
12/16 02:15:14 AM: Using weighting method: uniform, with normalized sample weights [0.3333 0.3333 0.3333] 
12/16 02:15:14 AM: Using loss scaling method: uniform, with weights {'mrpc': 1.0, 'sst': 1.0, 'wnli': 1.0}
12/16 02:15:14 AM: Beginning training. Stopping metric: macro_avg
12/16 02:15:14 AM: Beginning training. Stopping metric: macro_avg
12/16 02:15:14 AM: Stopped training after 8 validation checks
12/16 02:15:14 AM: Trained mrpc for 281 batches or 0.612 epochs
12/16 02:15:14 AM: Trained sst for 258 batches or 0.031 epochs
12/16 02:15:14 AM: Trained wnli for 261 batches or 3.263 epochs
12/16 02:15:14 AM: ***** VALIDATION RESULTS *****
12/16 02:15:14 AM: mrpc_acc_f1, 6, mrpc_loss: 0.58765, sst_loss: 0.65299, wnli_loss: 0.73734, macro_avg: 0.60740, micro_avg: 0.67410, mrpc_acc_f1: 0.75664, mrpc_accuracy: 0.70098, mrpc_f1: 0.81231, mrpc_precision: 0.71159, mrpc_recall: 0.94624, sst_accuracy: 0.65711, wnli_accuracy: 0.40845
12/16 02:15:14 AM: sst_accuracy, 7, mrpc_loss: 0.66879, sst_loss: 0.58105, wnli_loss: 0.73659, macro_avg: 0.60063, micro_avg: 0.69620, mrpc_acc_f1: 0.75139, mrpc_accuracy: 0.68873, mrpc_f1: 0.81406, mrpc_precision: 0.68812, mrpc_recall: 0.99642, sst_accuracy: 0.69839, wnli_accuracy: 0.35211
12/16 02:15:14 AM: wnli_accuracy, 1, mrpc_loss: 0.60739, sst_loss: 0.71782, wnli_loss: 0.70720, macro_avg: 0.60760, micro_avg: 0.58443, mrpc_acc_f1: 0.75139, mrpc_accuracy: 0.68873, mrpc_f1: 0.81406, mrpc_precision: 0.68812, mrpc_recall: 0.99642, sst_accuracy: 0.50803, wnli_accuracy: 0.56338
12/16 02:15:14 AM: micro_avg, 8, mrpc_loss: 0.58726, sst_loss: 0.56924, wnli_loss: 0.71919, macro_avg: 0.62468, micro_avg: 0.70042, mrpc_acc_f1: 0.75311, mrpc_accuracy: 0.69363, mrpc_f1: 0.81259, mrpc_precision: 0.69845, mrpc_recall: 0.97133, sst_accuracy: 0.69839, wnli_accuracy: 0.42254
12/16 02:15:14 AM: macro_avg, 2, mrpc_loss: 0.70026, sst_loss: 0.64640, wnli_loss: 0.69511, macro_avg: 0.65006, micro_avg: 0.66780, mrpc_acc_f1: 0.74803, mrpc_accuracy: 0.68382, mrpc_f1: 0.81223, mrpc_precision: 0.68382, mrpc_recall: 1.00000, sst_accuracy: 0.63876, wnli_accuracy: 0.56338
12/16 02:15:15 AM: Found no task-specific parameters to skip for task: wmt14_en_de
12/16 02:15:15 AM: Loaded model state from outputs/wmt/foobar/model_state_main_epoch_2.best_macro.th
12/16 02:15:15 AM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 346, in <module>
    main(sys.argv[1:])
  File "main.py", line 291, in main
    pred_module = getattr(model, "%s_mdl" % task.name)
  File "/home/pm2758/anaconda3/envs/jiant/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __getattr__
    type(self).__name__, name))
AttributeError: 'MultiTaskModel' object has no attribute 'wmt14_en_de_mdl'
Traceback (most recent call last):
  File "main.py", line 356, in <module>
    raise e  # re-raise exception, in case debugger is attached.
  File "main.py", line 346, in <module>
    main(sys.argv[1:])
  File "main.py", line 291, in main
    pred_module = getattr(model, "%s_mdl" % task.name)
  File "/home/pm2758/anaconda3/envs/jiant/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __getattr__
    type(self).__name__, name))
AttributeError: 'MultiTaskModel' object has no attribute 'wmt14_en_de_mdl'
