12/15 10:24:25 PM: fastText library not found!
12/15 10:24:26 PM: Loading config from config/iwslt.conf
12/15 10:24:26 PM: Config overrides: exp_name = iwslt, run_name = iwslt, d_hid = 256
12/15 10:24:26 PM: Waiting on git info....
12/15 10:24:26 PM: Git branch: master
12/15 10:24:26 PM: Git SHA: 58e9079f6a11a6ebd29edc0fe3d4fec403c7819c
12/15 10:24:26 PM: Parsed args: 
{
  "allow_missing_task_map": 0,
  "allow_reuse_of_pretraining_parameters": 0,
  "allow_untrained_encoder_parameters": 0,
  "batch_size": 8,
  "bidirectional": 1,
  "bpp_base": 1,
  "char_embs": 0,
  "char_filter_sizes": "2,3,4,5",
  "classifier": "mlp",
  "classifier_dropout": 0.2,
  "classifier_hid_dim": 32,
  "classifier_loss_fn": "",
  "classifier_span_pooling": "x,y",
  "cola": {},
  "cola_classifier_dropout": 0.2,
  "cola_classifier_hid_dim": 256,
  "cola_d_proj": 256,
  "cola_lr": 0.0003,
  "cola_val_interval": 100,
  "cove": 0,
  "cove_fine_tune": 0,
  "cuda": 0,
  "d_char": 100,
  "d_ff": 2048,
  "d_hid": 256,
  "d_hid_attn": 512,
  "d_proj": 512,
  "d_tproj": 64,
  "d_word": 300,
  "data_dir": "data",
  "dec_val_scale": 250,
  "do_full_eval": 1,
  "do_pretrain": 1,
  "do_target_task_training": 1,
  "dropout": 0.2,
  "dropout_embs": 0.2,
  "edgeprobe_cnn_context": 0,
  "edges-ccg-parse": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-ccg-tag": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-constituent-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-constituent-ontonotes-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-constituent-ptb": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-coref-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-coref-ontonotes-conll": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-coref-ontonotes-conll-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-dep-labeling": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-dep-labeling-ewt": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-dep-labeling-ewt-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-dpr": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-dpr-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-ner-conll2003": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 250
  },
  "edges-ner-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-ner-ontonotes-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-nonterminal-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-pos-ontonotes": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 250,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-spr1": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-spr1-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-spr2": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-spr2-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 100
  },
  "edges-srl-conll2005": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "edges-srl-conll2012": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-srl-conll2012-openai": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 1000
  },
  "edges-tmpl": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 256,
    "classifier_loss_fn": "sigmoid",
    "classifier_span_pooling": "attn",
    "max_vals": 100,
    "pair_attn": 0,
    "val_interval": 500
  },
  "elmo": 1,
  "elmo_chars_only": 1,
  "elmo_weight_file_path": "none",
  "eval_data_fraction": 1,
  "eval_max_vals": 10,
  "eval_val_interval": 10,
  "exp_dir": "outputs/iwslt/",
  "exp_name": "iwslt",
  "fastText": 0,
  "fastText_model_file": ".",
  "global_ro_exp_dir": "/nfs/jsalt/share/exp/demo",
  "grounded": {},
  "grounded_d_proj": 2048,
  "groundedsw": {},
  "groundedsw_d_proj": 2048,
  "is_probing_task": 0,
  "keep_all_checkpoints": 0,
  "load_eval_checkpoint": "none",
  "load_model": 0,
  "local_log_path": "outputs/iwslt/iwslt/log.log",
  "lr": 0.0001,
  "lr_decay_factor": 0.5,
  "lr_patience": 1,
  "max_char_v_size": 250,
  "max_grad_norm": 5.0,
  "max_seq_len": 10,
  "max_targ_word_v_size": 20000,
  "max_vals": 10,
  "max_word_v_size": 5000,
  "min_lr": 1e-06,
  "mnli": {},
  "mnli-alt": {},
  "mnli-alt_classifier_dropout": 0.2,
  "mnli-alt_classifier_hid_dim": 512,
  "mnli-alt_lr": 0.0003,
  "mnli-alt_pair_attn": 1,
  "mnli-alt_val_interval": 1000,
  "mnli-diagnostic": {
    "use_classifier": "mnli"
  },
  "mnli_classifier_dropout": 0.2,
  "mnli_classifier_hid_dim": 512,
  "mnli_lr": 0.0003,
  "mnli_pair_attn": 1,
  "mnli_val_interval": 1000,
  "mrpc": {
    "classifier_dropout": 0.1,
    "classifier_hid_dim": 256,
    "max_vals": 8,
    "val_interval": 1
  },
  "mrpc_classifier_dropout": 0.2,
  "mrpc_classifier_hid_dim": 256,
  "mrpc_d_proj": 256,
  "mrpc_lr": 0.0003,
  "mrpc_pair_attn": 0,
  "mrpc_val_interval": 100,
  "n_char_filters": 100,
  "n_heads": 8,
  "n_layers_enc": 1,
  "n_layers_highway": 0,
  "nli-prob": {
    "probe_path": ""
  },
  "openai_embeddings_mode": "none",
  "openai_transformer": 0,
  "openai_transformer_ckpt": "",
  "openai_transformer_fine_tune": 0,
  "optimizer": "adam",
  "pair_attn": 1,
  "patience": 5,
  "pretrain_tasks": "sst,mrpc,wnli",
  "project_dir": "outputs",
  "qnli": {},
  "qnli-alt": {},
  "qnli-alt_classifier_dropout": 0.2,
  "qnli-alt_classifier_hid_dim": 512,
  "qnli-alt_lr": 0.0003,
  "qnli-alt_pair_attn": 1,
  "qnli-alt_val_interval": 1000,
  "qnli_classifier_dropout": 0.2,
  "qnli_classifier_hid_dim": 512,
  "qnli_lr": 0.0003,
  "qnli_pair_attn": 1,
  "qnli_val_interval": 1000,
  "qqp": {},
  "qqp-alt": {},
  "qqp-alt_classifier_dropout": 0.2,
  "qqp-alt_classifier_hid_dim": 512,
  "qqp-alt_lr": 0.0003,
  "qqp-alt_pair_attn": 1,
  "qqp-alt_val_interval": 1000,
  "qqp_classifier_dropout": 0.2,
  "qqp_classifier_hid_dim": 512,
  "qqp_lr": 0.0003,
  "qqp_pair_attn": 1,
  "qqp_val_interval": 1000,
  "random_seed": 42,
  "reindex_tasks": "",
  "reload_indexing": 0,
  "reload_tasks": 0,
  "reload_vocab": 0,
  "remote_log_name": "iwslt__iwslt",
  "rte": {},
  "rte_classifier_dropout": 0.4,
  "rte_classifier_hid_dim": 128,
  "rte_d_proj": 128,
  "rte_lr": 0.0003,
  "rte_pair_attn": 0,
  "rte_val_interval": 100,
  "run_dir": "outputs/iwslt/iwslt",
  "run_name": "iwslt",
  "s2s": {
    "attention": "bilinear",
    "d_hid_dec": 1024,
    "n_layers_dec": 1,
    "output_proj_input_dim": 1024,
    "target_embedding_dim": 300
  },
  "scaling_method": "uniform",
  "scheduler_threshold": 0.0001,
  "sent_enc": "rnn",
  "sep_embs_for_skip": 0,
  "shared_optimizer": 1,
  "shared_pair_attn": 0,
  "skip_embs": 1,
  "sst": {},
  "sst_classifier_dropout": 0.2,
  "sst_classifier_hid_dim": 256,
  "sst_d_proj": 256,
  "sst_lr": 0.0003,
  "sst_val_interval": 100,
  "sts-b": {
    "classifier_dropout": 0.3,
    "classifier_hid_dim": 512,
    "max_vals": 16,
    "pair_attn": 0,
    "val_interval": 100
  },
  "sts-b-alt": {},
  "sts-b-alt_classifier_dropout": 0.2,
  "sts-b-alt_classifier_hid_dim": 512,
  "sts-b-alt_lr": 0.0003,
  "sts-b-alt_pair_attn": 1,
  "sts-b-alt_val_interval": 1000,
  "sts-b_classifier_dropout": 0.2,
  "sts-b_classifier_hid_dim": 512,
  "sts-b_lr": 0.0003,
  "sts-b_pair_attn": 1,
  "sts-b_val_interval": 1000,
  "target_tasks": "iwslt",
  "track_batch_utilization": 0,
  "trainer_type": "sampling",
  "training_data_fraction": 1,
  "use_classifier": "",
  "val_data_limit": 5000,
  "val_interval": 100,
  "warmup": 4000,
  "weighting_method": "uniform",
  "wnli": {},
  "wnli_classifier_dropout": 0.4,
  "wnli_classifier_hid_dim": 128,
  "wnli_d_proj": 128,
  "wnli_lr": 0.0003,
  "wnli_pair_attn": 0,
  "wnli_val_interval": 100,
  "word_embs": "none",
  "word_embs_file": "embeddings/glove.840B.300d.txt",
  "write_preds": 0,
  "write_strict_glue_format": 0
}
12/15 10:24:26 PM: Saved config to outputs/iwslt/iwslt/params.conf
12/15 10:24:26 PM: Using random seed 42
12/15 10:24:26 PM: Using GPU 0
12/15 10:24:26 PM: Loading tasks...
12/15 10:24:26 PM: Writing pre-preprocessed tasks to outputs/iwslt/
12/15 10:24:26 PM: 	Loaded existing task iwslt
12/15 10:24:27 PM: 	Task 'iwslt': train=150849 val=1525 test=1525
12/15 10:24:27 PM: 	Creating task mrpc from scratch
12/15 10:24:30 PM: 	Finished loading MRPC data.
12/15 10:24:30 PM: 	Task 'mrpc': train=3668 val=408 test=1725
12/15 10:24:30 PM: 	Creating task sst from scratch
12/15 10:24:35 PM: 	Finished loading SST data.
12/15 10:24:36 PM: 	Task 'sst': train=67349 val=872 test=1821
12/15 10:24:36 PM: 	Creating task wnli from scratch
12/15 10:24:36 PM: 	Finished loading Winograd.
12/15 10:24:36 PM: 	Task 'wnli': train=635 val=71 test=146
12/15 10:24:36 PM: 	Finished loading tasks: iwslt mrpc sst wnli.
12/15 10:24:36 PM: 	Building vocab from scratch
12/15 10:24:36 PM: 	Counting words for task: 'iwslt'
12/15 10:25:12 PM: 	Counting words for task: 'mrpc'
12/15 10:25:13 PM: 	Counting words for task: 'sst'
12/15 10:25:13 PM: 	Counting words for task: 'wnli'
12/15 10:25:13 PM: 	Finished counting words
12/15 10:25:13 PM: 	Task 'iwslt': adding vocab namespace 'iwslt_tokens'
12/15 10:25:49 PM: 	Saved vocab to outputs/iwslt/vocab
12/15 10:25:49 PM: Loading token dictionary from outputs/iwslt/vocab.
12/15 10:25:49 PM: 	Loaded vocab from outputs/iwslt/vocab
12/15 10:25:49 PM: 	Vocab namespace chars: size 115
12/15 10:25:49 PM: 	Vocab namespace iwslt_tokens: size 20002
12/15 10:25:49 PM: 	Vocab namespace tokens: size 5002
12/15 10:25:49 PM: 	Finished building vocab.
12/15 10:25:49 PM: 	Task 'iwslt', split 'train': indexing from scratch
12/15 10:26:52 PM: 	Task 'iwslt', split 'train': saved 150849 instances to outputs/iwslt/preproc/iwslt__train_data
12/15 10:26:52 PM: 	Task 'iwslt', split 'val': indexing from scratch
12/15 10:26:53 PM: 	Task 'iwslt', split 'val': saved 1525 instances to outputs/iwslt/preproc/iwslt__val_data
12/15 10:26:53 PM: 	Task 'iwslt', split 'test': indexing from scratch
12/15 10:26:53 PM: 	Task 'iwslt', split 'test': saved 1525 instances to outputs/iwslt/preproc/iwslt__test_data
12/15 10:26:53 PM: 	Task 'iwslt': cleared in-memory data.
12/15 10:26:53 PM: 	Task 'mrpc', split 'train': indexing from scratch
12/15 10:26:53 PM: Your label namespace was 'idxs'. We recommend you use a namespace ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by default to your vocabulary.  See documentation for `non_padded_namespaces` parameter in Vocabulary.
12/15 10:26:54 PM: 	Task 'mrpc', split 'train': saved 3668 instances to outputs/iwslt/preproc/mrpc__train_data
12/15 10:26:54 PM: 	Task 'mrpc', split 'val': indexing from scratch
12/15 10:26:54 PM: 	Task 'mrpc', split 'val': saved 408 instances to outputs/iwslt/preproc/mrpc__val_data
12/15 10:26:54 PM: 	Task 'mrpc', split 'test': indexing from scratch
12/15 10:26:54 PM: 	Task 'mrpc', split 'test': saved 1725 instances to outputs/iwslt/preproc/mrpc__test_data
12/15 10:26:54 PM: 	Task 'mrpc': cleared in-memory data.
12/15 10:26:54 PM: 	Task 'sst', split 'train': indexing from scratch
12/15 10:27:01 PM: 	Task 'sst', split 'train': saved 67349 instances to outputs/iwslt/preproc/sst__train_data
12/15 10:27:01 PM: 	Task 'sst', split 'val': indexing from scratch
12/15 10:27:01 PM: 	Task 'sst', split 'val': saved 872 instances to outputs/iwslt/preproc/sst__val_data
12/15 10:27:01 PM: 	Task 'sst', split 'test': indexing from scratch
12/15 10:27:01 PM: 	Task 'sst', split 'test': saved 1821 instances to outputs/iwslt/preproc/sst__test_data
12/15 10:27:01 PM: 	Task 'sst': cleared in-memory data.
12/15 10:27:01 PM: 	Task 'wnli', split 'train': indexing from scratch
12/15 10:27:01 PM: 	Task 'wnli', split 'train': saved 635 instances to outputs/iwslt/preproc/wnli__train_data
12/15 10:27:01 PM: 	Task 'wnli', split 'val': indexing from scratch
12/15 10:27:01 PM: 	Task 'wnli', split 'val': saved 71 instances to outputs/iwslt/preproc/wnli__val_data
12/15 10:27:01 PM: 	Task 'wnli', split 'test': indexing from scratch
12/15 10:27:01 PM: 	Task 'wnli', split 'test': saved 146 instances to outputs/iwslt/preproc/wnli__test_data
12/15 10:27:01 PM: 	Task 'wnli': cleared in-memory data.
12/15 10:27:01 PM: 	Finished indexing tasks
12/15 10:27:01 PM: 	Lazy-loading indexed data for task='iwslt' from outputs/iwslt/preproc
12/15 10:27:01 PM: 	Lazy-loading indexed data for task='mrpc' from outputs/iwslt/preproc
12/15 10:27:01 PM: 	Lazy-loading indexed data for task='sst' from outputs/iwslt/preproc
12/15 10:27:01 PM: 	Lazy-loading indexed data for task='wnli' from outputs/iwslt/preproc
12/15 10:27:01 PM: All tasks initialized with data iterators.
12/15 10:27:01 PM: 	  Training on sst, mrpc, wnli
12/15 10:27:01 PM: 	  Evaluating on iwslt
12/15 10:27:01 PM: 	Finished loading tasks in 155.531s
12/15 10:27:01 PM: 	 Tasks: ['iwslt', 'mrpc', 'sst', 'wnli']
12/15 10:27:01 PM: Building model...
12/15 10:27:01 PM: 	Not using word embeddings!
12/15 10:27:01 PM: 	Not using character embeddings!
12/15 10:27:01 PM: Loading ELMo from files:
12/15 10:27:01 PM: ELMO_OPT_PATH = https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json
12/15 10:27:01 PM: 	Using ELMo character CNN only!
12/15 10:27:01 PM: ELMO_WEIGHTS_PATH = https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5
12/15 10:27:06 PM: batch_first = True
12/15 10:27:06 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:06 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:06 PM: input_size = 512
12/15 10:27:06 PM: bidirectional = True
12/15 10:27:06 PM: hidden_size = 256
12/15 10:27:06 PM: num_layers = 1
12/15 10:27:06 PM: batch_first = True
12/15 10:27:06 PM: Initializing parameters
12/15 10:27:06 PM: Done initializing parameters; the following parameters are using their default initialization from their code
12/15 10:27:06 PM:    _phrase_layer._module.bias_hh_l0
12/15 10:27:06 PM:    _phrase_layer._module.bias_hh_l0_reverse
12/15 10:27:06 PM:    _phrase_layer._module.bias_ih_l0
12/15 10:27:06 PM:    _phrase_layer._module.bias_ih_l0_reverse
12/15 10:27:06 PM:    _phrase_layer._module.weight_hh_l0
12/15 10:27:06 PM:    _phrase_layer._module.weight_hh_l0_reverse
12/15 10:27:06 PM:    _phrase_layer._module.weight_ih_l0
12/15 10:27:06 PM:    _phrase_layer._module.weight_ih_l0_reverse
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo._char_embedding_weights
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo._highways._layers.0.bias
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo._highways._layers.0.weight
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo._highways._layers.1.bias
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo._highways._layers.1.weight
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo._projection.bias
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo._projection.weight
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_0.bias
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_0.weight
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_1.bias
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_1.weight
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_2.bias
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_2.weight
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_3.bias
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_3.weight
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_4.bias
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_4.weight
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_5.bias
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_5.weight
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_6.bias
12/15 10:27:06 PM:    _text_field_embedder.token_embedder_elmo.char_conv_6.weight
12/15 10:27:06 PM: Using BiLSTM architecture for shared encoder!
12/15 10:27:06 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:06 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:06 PM: cls_type = mlp
12/15 10:27:06 PM: d_hid = 32
12/15 10:27:06 PM: d_proj = 512
12/15 10:27:06 PM: shared_pair_attn = 0
12/15 10:27:06 PM: attn = 1
12/15 10:27:06 PM: d_hid_attn = 512
12/15 10:27:06 PM: dropout = 0.2
12/15 10:27:06 PM: cls_loss_fn = 
12/15 10:27:06 PM: cls_span_pooling = x,y
12/15 10:27:06 PM: edgeprobe_cnn_context = 0
12/15 10:27:06 PM: use_classifier = iwslt
12/15 10:27:06 PM: 	Task 'iwslt' params: {
  "cls_type": "mlp",
  "d_hid": 32,
  "d_proj": 512,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "",
  "cls_span_pooling": "x,y",
  "edgeprobe_cnn_context": 0,
  "use_classifier": "iwslt"
}
12/15 10:27:06 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:06 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:06 PM: cls_type = mlp
12/15 10:27:06 PM: d_hid = 128
12/15 10:27:06 PM: d_proj = 128
12/15 10:27:06 PM: shared_pair_attn = 0
12/15 10:27:06 PM: attn = 0
12/15 10:27:06 PM: d_hid_attn = 512
12/15 10:27:06 PM: dropout = 0.4
12/15 10:27:06 PM: cls_loss_fn = 
12/15 10:27:06 PM: cls_span_pooling = x,y
12/15 10:27:06 PM: edgeprobe_cnn_context = 0
12/15 10:27:06 PM: use_classifier = wnli
12/15 10:27:06 PM: 	Task 'wnli' params: {
  "cls_type": "mlp",
  "d_hid": 128,
  "d_proj": 128,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.4,
  "cls_loss_fn": "",
  "cls_span_pooling": "x,y",
  "edgeprobe_cnn_context": 0,
  "use_classifier": "wnli"
}
12/15 10:27:06 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:06 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:06 PM: cls_type = mlp
12/15 10:27:06 PM: d_hid = 256
12/15 10:27:06 PM: d_proj = 256
12/15 10:27:06 PM: shared_pair_attn = 0
12/15 10:27:06 PM: attn = 1
12/15 10:27:06 PM: d_hid_attn = 512
12/15 10:27:06 PM: dropout = 0.2
12/15 10:27:06 PM: cls_loss_fn = 
12/15 10:27:06 PM: cls_span_pooling = x,y
12/15 10:27:06 PM: edgeprobe_cnn_context = 0
12/15 10:27:06 PM: use_classifier = sst
12/15 10:27:06 PM: 	Task 'sst' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "d_proj": 256,
  "shared_pair_attn": 0,
  "attn": 1,
  "d_hid_attn": 512,
  "dropout": 0.2,
  "cls_loss_fn": "",
  "cls_span_pooling": "x,y",
  "edgeprobe_cnn_context": 0,
  "use_classifier": "sst"
}
12/15 10:27:06 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:06 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:06 PM: cls_type = mlp
12/15 10:27:06 PM: d_hid = 256
12/15 10:27:06 PM: d_proj = 256
12/15 10:27:06 PM: shared_pair_attn = 0
12/15 10:27:06 PM: attn = 0
12/15 10:27:06 PM: d_hid_attn = 512
12/15 10:27:06 PM: dropout = 0.1
12/15 10:27:06 PM: cls_loss_fn = 
12/15 10:27:06 PM: cls_span_pooling = x,y
12/15 10:27:06 PM: edgeprobe_cnn_context = 0
12/15 10:27:06 PM: use_classifier = mrpc
12/15 10:27:06 PM: 	Task 'mrpc' params: {
  "cls_type": "mlp",
  "d_hid": 256,
  "d_proj": 256,
  "shared_pair_attn": 0,
  "attn": 0,
  "d_hid_attn": 512,
  "dropout": 0.1,
  "cls_loss_fn": "",
  "cls_span_pooling": "x,y",
  "edgeprobe_cnn_context": 0,
  "use_classifier": "mrpc"
}
12/15 10:27:06 PM: using bilinear attention
12/15 10:27:13 PM: MultiTaskModel(
  (sent_encoder): SentenceEncoder(
    (_text_field_embedder): ElmoTextFieldEmbedder(
      (token_embedder_elmo): ElmoCharacterEncoder(
        (char_conv_0): Conv1d(16, 32, kernel_size=(1,), stride=(1,))
        (char_conv_1): Conv1d(16, 32, kernel_size=(2,), stride=(1,))
        (char_conv_2): Conv1d(16, 64, kernel_size=(3,), stride=(1,))
        (char_conv_3): Conv1d(16, 128, kernel_size=(4,), stride=(1,))
        (char_conv_4): Conv1d(16, 256, kernel_size=(5,), stride=(1,))
        (char_conv_5): Conv1d(16, 512, kernel_size=(6,), stride=(1,))
        (char_conv_6): Conv1d(16, 1024, kernel_size=(7,), stride=(1,))
        (_highways): Highway(
          (_layers): ModuleList(
            (0): Linear(in_features=2048, out_features=4096, bias=True)
            (1): Linear(in_features=2048, out_features=4096, bias=True)
          )
        )
        (_projection): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
    (_highway_layer): TimeDistributed(
      (_module): Highway(
        (_layers): ModuleList()
      )
    )
    (_phrase_layer): PytorchSeq2SeqWrapper(
      (_module): LSTM(512, 256, batch_first=True, bidirectional=True)
    )
    (_dropout): Dropout(p=0.2)
  )
  (iwslt_decoder): Seq2SeqDecoder(
    (_target_embedder): Embedding()
    (_sent_pooler): Pooler(
      (project): Linear(in_features=1024, out_features=1024, bias=True)
    )
    (_decoder_attention): BilinearAttention()
    (_decoder_cell): LSTMCell(1324, 1024)
    (_output_projection_layer): Linear(in_features=1024, out_features=20002, bias=True)
    (_dropout): Dropout(p=0.2)
  )
  (mrpc_mdl): PairClassifier(
    (pooler): Pooler(
      (project): Linear(in_features=1024, out_features=256, bias=True)
    )
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=1024, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.1)
        (4): Linear(in_features=256, out_features=2, bias=True)
      )
    )
  )
  (sst_mdl): SingleClassifier(
    (pooler): Pooler(
      (project): Linear(in_features=1024, out_features=256, bias=True)
    )
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([256]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.2)
        (4): Linear(in_features=256, out_features=2, bias=True)
      )
    )
  )
  (wnli_mdl): PairClassifier(
    (pooler): Pooler(
      (project): Linear(in_features=1024, out_features=128, bias=True)
    )
    (classifier): Classifier(
      (classifier): Sequential(
        (0): Linear(in_features=512, out_features=128, bias=True)
        (1): Tanh()
        (2): LayerNorm(torch.Size([128]), eps=1e-05, elementwise_affine=True)
        (3): Dropout(p=0.4)
        (4): Linear(in_features=128, out_features=2, bias=True)
      )
    )
  )
)
12/15 10:27:13 PM: >> Trainable param sent_encoder._phrase_layer._module.weight_ih_l0: torch.Size([1024, 512]) = 524288
12/15 10:27:13 PM: >> Trainable param sent_encoder._phrase_layer._module.weight_hh_l0: torch.Size([1024, 256]) = 262144
12/15 10:27:13 PM: >> Trainable param sent_encoder._phrase_layer._module.bias_ih_l0: torch.Size([1024]) = 1024
12/15 10:27:13 PM: >> Trainable param sent_encoder._phrase_layer._module.bias_hh_l0: torch.Size([1024]) = 1024
12/15 10:27:13 PM: >> Trainable param sent_encoder._phrase_layer._module.weight_ih_l0_reverse: torch.Size([1024, 512]) = 524288
12/15 10:27:13 PM: >> Trainable param sent_encoder._phrase_layer._module.weight_hh_l0_reverse: torch.Size([1024, 256]) = 262144
12/15 10:27:13 PM: >> Trainable param sent_encoder._phrase_layer._module.bias_ih_l0_reverse: torch.Size([1024]) = 1024
12/15 10:27:13 PM: >> Trainable param sent_encoder._phrase_layer._module.bias_hh_l0_reverse: torch.Size([1024]) = 1024
12/15 10:27:13 PM: >> Trainable param iwslt_decoder._target_embedder.weight: torch.Size([20002, 300]) = 6000600
12/15 10:27:13 PM: >> Trainable param iwslt_decoder._sent_pooler.project.weight: torch.Size([1024, 1024]) = 1048576
12/15 10:27:13 PM: >> Trainable param iwslt_decoder._sent_pooler.project.bias: torch.Size([1024]) = 1024
12/15 10:27:13 PM: >> Trainable param iwslt_decoder._decoder_attention._weight_matrix: torch.Size([1024, 1024]) = 1048576
12/15 10:27:13 PM: >> Trainable param iwslt_decoder._decoder_attention._bias: torch.Size([1]) = 1
12/15 10:27:13 PM: >> Trainable param iwslt_decoder._decoder_cell.weight_ih: torch.Size([4096, 1324]) = 5423104
12/15 10:27:13 PM: >> Trainable param iwslt_decoder._decoder_cell.weight_hh: torch.Size([4096, 1024]) = 4194304
12/15 10:27:13 PM: >> Trainable param iwslt_decoder._decoder_cell.bias_ih: torch.Size([4096]) = 4096
12/15 10:27:13 PM: >> Trainable param iwslt_decoder._decoder_cell.bias_hh: torch.Size([4096]) = 4096
12/15 10:27:13 PM: >> Trainable param iwslt_decoder._output_projection_layer.weight: torch.Size([20002, 1024]) = 20482048
12/15 10:27:13 PM: >> Trainable param iwslt_decoder._output_projection_layer.bias: torch.Size([20002]) = 20002
12/15 10:27:13 PM: >> Trainable param mrpc_mdl.pooler.project.weight: torch.Size([256, 1024]) = 262144
12/15 10:27:13 PM: >> Trainable param mrpc_mdl.pooler.project.bias: torch.Size([256]) = 256
12/15 10:27:13 PM: >> Trainable param mrpc_mdl.classifier.classifier.0.weight: torch.Size([256, 1024]) = 262144
12/15 10:27:13 PM: >> Trainable param mrpc_mdl.classifier.classifier.0.bias: torch.Size([256]) = 256
12/15 10:27:13 PM: >> Trainable param mrpc_mdl.classifier.classifier.2.weight: torch.Size([256]) = 256
12/15 10:27:13 PM: >> Trainable param mrpc_mdl.classifier.classifier.2.bias: torch.Size([256]) = 256
12/15 10:27:13 PM: >> Trainable param mrpc_mdl.classifier.classifier.4.weight: torch.Size([2, 256]) = 512
12/15 10:27:13 PM: >> Trainable param mrpc_mdl.classifier.classifier.4.bias: torch.Size([2]) = 2
12/15 10:27:13 PM: >> Trainable param sst_mdl.pooler.project.weight: torch.Size([256, 1024]) = 262144
12/15 10:27:13 PM: >> Trainable param sst_mdl.pooler.project.bias: torch.Size([256]) = 256
12/15 10:27:13 PM: >> Trainable param sst_mdl.classifier.classifier.0.weight: torch.Size([256, 256]) = 65536
12/15 10:27:13 PM: >> Trainable param sst_mdl.classifier.classifier.0.bias: torch.Size([256]) = 256
12/15 10:27:13 PM: >> Trainable param sst_mdl.classifier.classifier.2.weight: torch.Size([256]) = 256
12/15 10:27:13 PM: >> Trainable param sst_mdl.classifier.classifier.2.bias: torch.Size([256]) = 256
12/15 10:27:13 PM: >> Trainable param sst_mdl.classifier.classifier.4.weight: torch.Size([2, 256]) = 512
12/15 10:27:13 PM: >> Trainable param sst_mdl.classifier.classifier.4.bias: torch.Size([2]) = 2
12/15 10:27:13 PM: >> Trainable param wnli_mdl.pooler.project.weight: torch.Size([128, 1024]) = 131072
12/15 10:27:13 PM: >> Trainable param wnli_mdl.pooler.project.bias: torch.Size([128]) = 128
12/15 10:27:13 PM: >> Trainable param wnli_mdl.classifier.classifier.0.weight: torch.Size([128, 512]) = 65536
12/15 10:27:13 PM: >> Trainable param wnli_mdl.classifier.classifier.0.bias: torch.Size([128]) = 128
12/15 10:27:13 PM: >> Trainable param wnli_mdl.classifier.classifier.2.weight: torch.Size([128]) = 128
12/15 10:27:13 PM: >> Trainable param wnli_mdl.classifier.classifier.2.bias: torch.Size([128]) = 128
12/15 10:27:13 PM: >> Trainable param wnli_mdl.classifier.classifier.4.weight: torch.Size([2, 128]) = 256
12/15 10:27:13 PM: >> Trainable param wnli_mdl.classifier.classifier.4.bias: torch.Size([2]) = 2
12/15 10:27:13 PM: Total number of parameters: 58893665 (5.88937e+07)
12/15 10:27:13 PM: Number of trainable parameters: 40855809 (4.08558e+07)
12/15 10:27:13 PM: 	Finished building model in 11.820s
12/15 10:27:13 PM: Will run the following steps:
Training model on tasks: sst,mrpc,wnli
Re-training model for individual eval tasks
Evaluating model on tasks: iwslt
12/15 10:27:13 PM: Training...
12/15 10:27:13 PM: 	Using ReduceLROnPlateau scheduler!
12/15 10:27:13 PM: patience = 5
12/15 10:27:13 PM: val_interval = 100
12/15 10:27:13 PM: max_vals = 10
12/15 10:27:13 PM: cuda_device = 0
12/15 10:27:13 PM: grad_norm = 5.0
12/15 10:27:13 PM: grad_clipping = None
12/15 10:27:13 PM: lr_decay = 0.99
12/15 10:27:13 PM: min_lr = 1e-06
12/15 10:27:13 PM: keep_all_checkpoints = 0
12/15 10:27:13 PM: val_data_limit = 5000
12/15 10:27:13 PM: dec_val_scale = 250
12/15 10:27:13 PM: training_data_fraction = 1
12/15 10:27:13 PM: type = adam
12/15 10:27:13 PM: parameter_groups = None
12/15 10:27:13 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:13 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:13 PM: lr = 0.0001
12/15 10:27:13 PM: weight_decay = 0
12/15 10:27:13 PM: amsgrad = True
12/15 10:27:13 PM: type = reduce_on_plateau
12/15 10:27:13 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:13 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:13 PM: mode = max
12/15 10:27:13 PM: factor = 0.5
12/15 10:27:13 PM: patience = 1
12/15 10:27:13 PM: threshold = 0.0001
12/15 10:27:13 PM: threshold_mode = abs
12/15 10:27:13 PM: verbose = True
12/15 10:27:13 PM: type = adam
12/15 10:27:13 PM: parameter_groups = None
12/15 10:27:13 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:13 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:13 PM: lr = 0.0001
12/15 10:27:13 PM: weight_decay = 0
12/15 10:27:13 PM: amsgrad = True
12/15 10:27:13 PM: type = reduce_on_plateau
12/15 10:27:13 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:13 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:13 PM: mode = max
12/15 10:27:13 PM: factor = 0.5
12/15 10:27:13 PM: patience = 1
12/15 10:27:13 PM: threshold = 0.0001
12/15 10:27:13 PM: threshold_mode = abs
12/15 10:27:13 PM: verbose = True
12/15 10:27:13 PM: type = adam
12/15 10:27:13 PM: parameter_groups = None
12/15 10:27:13 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:13 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:13 PM: lr = 0.0001
12/15 10:27:13 PM: weight_decay = 0
12/15 10:27:13 PM: amsgrad = True
12/15 10:27:13 PM: type = reduce_on_plateau
12/15 10:27:13 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:13 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:13 PM: mode = max
12/15 10:27:13 PM: factor = 0.5
12/15 10:27:13 PM: patience = 1
12/15 10:27:13 PM: threshold = 0.0001
12/15 10:27:13 PM: threshold_mode = abs
12/15 10:27:13 PM: verbose = True
12/15 10:27:13 PM: type = adam
12/15 10:27:13 PM: parameter_groups = None
12/15 10:27:13 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:13 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:13 PM: lr = 0.0001
12/15 10:27:13 PM: weight_decay = 0
12/15 10:27:13 PM: amsgrad = True
12/15 10:27:13 PM: type = reduce_on_plateau
12/15 10:27:13 PM: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.
12/15 10:27:13 PM: CURRENTLY DEFINED PARAMETERS: 
12/15 10:27:13 PM: mode = max
12/15 10:27:13 PM: factor = 0.5
12/15 10:27:13 PM: patience = 1
12/15 10:27:13 PM: threshold = 0.0001
12/15 10:27:13 PM: threshold_mode = abs
12/15 10:27:13 PM: verbose = True
12/15 10:27:13 PM: Not loading.
12/15 10:27:13 PM: Training examples per task: {'mrpc': 3668, 'sst': 67349, 'wnli': 635}
12/15 10:27:13 PM: Sampling tasks uniformly.
12/15 10:27:13 PM: Using weighting method: uniform, with normalized sample weights [0.3333 0.3333 0.3333] 
12/15 10:27:13 PM: Using loss scaling method: uniform, with weights {'mrpc': 1.0, 'sst': 1.0, 'wnli': 1.0}
12/15 10:27:13 PM: Beginning training. Stopping metric: macro_avg
12/15 10:27:13 PM: Beginning training. Stopping metric: macro_avg
12/15 10:27:17 PM: ***** Pass 100 / Epoch 1 *****
12/15 10:27:17 PM: mrpc: trained on 38 batches, 0.083 epochs
12/15 10:27:17 PM: sst: trained on 33 batches, 0.004 epochs
12/15 10:27:17 PM: wnli: trained on 29 batches, 0.362 epochs
12/15 10:27:17 PM: Validating...
12/15 10:27:18 PM: Best model found for mrpc.
12/15 10:27:18 PM: Best model found for sst.
12/15 10:27:18 PM: Best model found for wnli.
12/15 10:27:18 PM: Best model found for micro.
12/15 10:27:18 PM: Best model found for macro.
12/15 10:27:18 PM: Advancing scheduler.
12/15 10:27:18 PM: 	Best macro_avg: 0.610
12/15 10:27:18 PM: 	# bad epochs: 0
12/15 10:27:18 PM: Statistic: mrpc_loss
12/15 10:27:18 PM: 	training: 0.698018
12/15 10:27:18 PM: 	validation: 0.604168
12/15 10:27:18 PM: Statistic: sst_loss
12/15 10:27:18 PM: 	training: 0.743301
12/15 10:27:18 PM: 	validation: 0.725667
12/15 10:27:18 PM: Statistic: wnli_loss
12/15 10:27:18 PM: 	training: 0.755654
12/15 10:27:18 PM: 	validation: 0.706789
12/15 10:27:18 PM: Statistic: macro_avg
12/15 10:27:18 PM: 	validation: 0.609984
12/15 10:27:18 PM: Statistic: micro_avg
12/15 10:27:18 PM: 	validation: 0.588955
12/15 10:27:18 PM: Statistic: mrpc_acc_f1
12/15 10:27:18 PM: 	training: 0.712129
12/15 10:27:18 PM: 	validation: 0.751662
12/15 10:27:18 PM: Statistic: mrpc_accuracy
12/15 10:27:18 PM: 	training: 0.653333
12/15 10:27:18 PM: 	validation: 0.688725
12/15 10:27:18 PM: Statistic: mrpc_f1
12/15 10:27:18 PM: 	training: 0.770925
12/15 10:27:18 PM: 	validation: 0.814599
12/15 10:27:18 PM: Statistic: mrpc_precision
12/15 10:27:18 PM: 	training: 0.700000
12/15 10:27:18 PM: 	validation: 0.687192
12/15 10:27:18 PM: Statistic: mrpc_recall
12/15 10:27:18 PM: 	training: 0.857843
12/15 10:27:18 PM: 	validation: 1.000000
12/15 10:27:18 PM: Statistic: sst_accuracy
12/15 10:27:18 PM: 	training: 0.526515
12/15 10:27:18 PM: 	validation: 0.514908
12/15 10:27:18 PM: Statistic: wnli_accuracy
12/15 10:27:18 PM: 	training: 0.546256
12/15 10:27:18 PM: 	validation: 0.563380
12/15 10:27:18 PM: global_lr: 0.000100
12/15 10:27:19 PM: Saved files to outputs/iwslt/iwslt
12/15 10:27:21 PM: ***** Pass 200 / Epoch 2 *****
12/15 10:27:21 PM: mrpc: trained on 39 batches, 0.085 epochs
12/15 10:27:21 PM: sst: trained on 25 batches, 0.003 epochs
12/15 10:27:21 PM: wnli: trained on 36 batches, 0.450 epochs
12/15 10:27:21 PM: Validating...
12/15 10:27:23 PM: Best model found for sst.
12/15 10:27:23 PM: Best model found for micro.
12/15 10:27:23 PM: Best model found for macro.
12/15 10:27:23 PM: Advancing scheduler.
12/15 10:27:23 PM: 	Best macro_avg: 0.642
12/15 10:27:23 PM: 	# bad epochs: 0
12/15 10:27:23 PM: Statistic: mrpc_loss
12/15 10:27:23 PM: 	training: 0.648138
12/15 10:27:23 PM: 	validation: 0.692594
12/15 10:27:23 PM: Statistic: sst_loss
12/15 10:27:23 PM: 	training: 0.737953
12/15 10:27:23 PM: 	validation: 0.645026
12/15 10:27:23 PM: Statistic: wnli_loss
12/15 10:27:23 PM: 	training: 0.784951
12/15 10:27:23 PM: 	validation: 0.694675
12/15 10:27:23 PM: Statistic: macro_avg
12/15 10:27:23 PM: 	validation: 0.641813
12/15 10:27:23 PM: Statistic: micro_avg
12/15 10:27:23 PM: 	validation: 0.668538
12/15 10:27:23 PM: Statistic: mrpc_acc_f1
12/15 10:27:23 PM: 	training: 0.727867
12/15 10:27:23 PM: 	validation: 0.748025
12/15 10:27:23 PM: Statistic: mrpc_accuracy
12/15 10:27:23 PM: 	training: 0.669872
12/15 10:27:23 PM: 	validation: 0.683824
12/15 10:27:23 PM: Statistic: mrpc_f1
12/15 10:27:23 PM: 	training: 0.785863
12/15 10:27:23 PM: 	validation: 0.812227
12/15 10:27:23 PM: Statistic: mrpc_precision
12/15 10:27:23 PM: 	training: 0.697417
12/15 10:27:23 PM: 	validation: 0.683824
12/15 10:27:23 PM: Statistic: mrpc_recall
12/15 10:27:23 PM: 	training: 0.900000
12/15 10:27:23 PM: 	validation: 1.000000
12/15 10:27:23 PM: Statistic: sst_accuracy
12/15 10:27:23 PM: 	training: 0.515000
12/15 10:27:23 PM: 	validation: 0.642202
12/15 10:27:23 PM: Statistic: wnli_accuracy
12/15 10:27:23 PM: 	training: 0.420139
12/15 10:27:23 PM: 	validation: 0.535211
12/15 10:27:23 PM: global_lr: 0.000100
12/15 10:27:24 PM: Saved files to outputs/iwslt/iwslt
12/15 10:27:24 PM: Update 201: task sst, batch 1 (59): accuracy: 0.8750, sst_loss: 0.4209 ||
12/15 10:27:24 PM: Update 202: task wnli, batch 1 (66): accuracy: 0.7500, wnli_loss: 0.5982 ||
12/15 10:27:24 PM: Update 203: task mrpc, batch 1 (78): acc_f1: 0.8036, accuracy: 0.7500, f1: 0.8571, precision: 0.7500, recall: 1.0000, mrpc_loss: 0.7299 ||
12/15 10:27:26 PM: ***** Pass 300 / Epoch 3 *****
12/15 10:27:26 PM: mrpc: trained on 39 batches, 0.085 epochs
12/15 10:27:26 PM: sst: trained on 28 batches, 0.003 epochs
12/15 10:27:26 PM: wnli: trained on 33 batches, 0.412 epochs
12/15 10:27:26 PM: Validating...
12/15 10:27:27 PM: Advancing scheduler.
12/15 10:27:27 PM: 	Best macro_avg: 0.642
12/15 10:27:27 PM: 	# bad epochs: 1
12/15 10:27:27 PM: Statistic: mrpc_loss
12/15 10:27:27 PM: 	training: 0.666156
12/15 10:27:27 PM: 	validation: 0.683203
12/15 10:27:27 PM: Statistic: sst_loss
12/15 10:27:27 PM: 	training: 0.585797
12/15 10:27:27 PM: 	validation: 0.645984
12/15 10:27:27 PM: Statistic: wnli_loss
12/15 10:27:27 PM: 	training: 0.732170
12/15 10:27:27 PM: 	validation: 0.724069
12/15 10:27:27 PM: Statistic: macro_avg
12/15 10:27:27 PM: 	validation: 0.539662
12/15 10:27:27 PM: Statistic: micro_avg
12/15 10:27:27 PM: 	validation: 0.597006
12/15 10:27:27 PM: Statistic: mrpc_acc_f1
12/15 10:27:27 PM: 	training: 0.710549
12/15 10:27:27 PM: 	validation: 0.574890
12/15 10:27:27 PM: Statistic: mrpc_accuracy
12/15 10:27:27 PM: 	training: 0.647436
12/15 10:27:27 PM: 	validation: 0.561275
12/15 10:27:27 PM: Statistic: mrpc_f1
12/15 10:27:27 PM: 	training: 0.773663
12/15 10:27:27 PM: 	validation: 0.588506
12/15 10:27:27 PM: Statistic: mrpc_precision
12/15 10:27:27 PM: 	training: 0.678700
12/15 10:27:27 PM: 	validation: 0.820513
12/15 10:27:27 PM: Statistic: mrpc_recall
12/15 10:27:27 PM: 	training: 0.899522
12/15 10:27:27 PM: 	validation: 0.458781
12/15 10:27:27 PM: Statistic: sst_accuracy
12/15 10:27:27 PM: 	training: 0.678571
12/15 10:27:27 PM: 	validation: 0.621560
12/15 10:27:27 PM: Statistic: wnli_accuracy
12/15 10:27:27 PM: 	training: 0.501931
12/15 10:27:27 PM: 	validation: 0.422535
12/15 10:27:27 PM: global_lr: 0.000100
12/15 10:27:27 PM: Saved files to outputs/iwslt/iwslt
12/15 10:27:29 PM: ***** Pass 400 / Epoch 4 *****
12/15 10:27:29 PM: mrpc: trained on 33 batches, 0.072 epochs
12/15 10:27:29 PM: sst: trained on 28 batches, 0.003 epochs
12/15 10:27:29 PM: wnli: trained on 39 batches, 0.487 epochs
12/15 10:27:29 PM: Validating...
12/15 10:27:31 PM: Best model found for mrpc.
12/15 10:27:31 PM: Best model found for sst.
12/15 10:27:31 PM: Best model found for micro.
12/15 10:27:31 PM: Best model found for macro.
12/15 10:27:31 PM: Advancing scheduler.
12/15 10:27:31 PM: 	Best macro_avg: 0.659
12/15 10:27:31 PM: 	# bad epochs: 0
12/15 10:27:31 PM: Statistic: mrpc_loss
12/15 10:27:31 PM: 	training: 0.638291
12/15 10:27:31 PM: 	validation: 0.636527
12/15 10:27:31 PM: Statistic: sst_loss
12/15 10:27:31 PM: 	training: 0.591068
12/15 10:27:31 PM: 	validation: 0.621894
12/15 10:27:31 PM: Statistic: wnli_loss
12/15 10:27:31 PM: 	training: 0.726933
12/15 10:27:31 PM: 	validation: 0.704870
12/15 10:27:31 PM: Statistic: macro_avg
12/15 10:27:31 PM: 	validation: 0.658981
12/15 10:27:31 PM: Statistic: micro_avg
12/15 10:27:31 PM: 	validation: 0.682579
12/15 10:27:31 PM: Statistic: mrpc_acc_f1
12/15 10:27:31 PM: 	training: 0.701816
12/15 10:27:31 PM: 	validation: 0.755306
12/15 10:27:31 PM: Statistic: mrpc_accuracy
12/15 10:27:31 PM: 	training: 0.647727
12/15 10:27:31 PM: 	validation: 0.693627
12/15 10:27:31 PM: Statistic: mrpc_f1
12/15 10:27:31 PM: 	training: 0.755906
12/15 10:27:31 PM: 	validation: 0.816984
12/15 10:27:31 PM: Statistic: mrpc_precision
12/15 10:27:31 PM: 	training: 0.705882
12/15 10:27:31 PM: 	validation: 0.690594
12/15 10:27:31 PM: Statistic: mrpc_recall
12/15 10:27:31 PM: 	training: 0.813559
12/15 10:27:31 PM: 	validation: 1.000000
12/15 10:27:31 PM: Statistic: sst_accuracy
12/15 10:27:31 PM: 	training: 0.647321
12/15 10:27:31 PM: 	validation: 0.658257
12/15 10:27:31 PM: Statistic: wnli_accuracy
12/15 10:27:31 PM: 	training: 0.532051
12/15 10:27:31 PM: 	validation: 0.563380
12/15 10:27:31 PM: global_lr: 0.000100
12/15 10:27:31 PM: Saved files to outputs/iwslt/iwslt
12/15 10:27:33 PM: ***** Pass 500 / Epoch 5 *****
12/15 10:27:33 PM: mrpc: trained on 33 batches, 0.072 epochs
12/15 10:27:33 PM: sst: trained on 40 batches, 0.005 epochs
12/15 10:27:33 PM: wnli: trained on 27 batches, 0.338 epochs
12/15 10:27:33 PM: Validating...
12/15 10:27:34 PM: Batch 39/109: accuracy: 0.6506, sst_loss: 0.6248 ||
12/15 10:27:34 PM: Batch 1/9: accuracy: 0.6250, wnli_loss: 0.6780 ||
12/15 10:27:34 PM: Best model found for sst.
12/15 10:27:34 PM: Best model found for micro.
12/15 10:27:34 PM: Advancing scheduler.
12/15 10:27:34 PM: 	Best macro_avg: 0.659
12/15 10:27:34 PM: 	# bad epochs: 1
12/15 10:27:34 PM: Statistic: mrpc_loss
12/15 10:27:34 PM: 	training: 0.666745
12/15 10:27:34 PM: 	validation: 0.604024
12/15 10:27:34 PM: Statistic: sst_loss
12/15 10:27:34 PM: 	training: 0.578032
12/15 10:27:34 PM: 	validation: 0.585807
12/15 10:27:34 PM: Statistic: wnli_loss
12/15 10:27:34 PM: 	training: 0.730141
12/15 10:27:34 PM: 	validation: 0.782962
12/15 10:27:34 PM: Statistic: macro_avg
12/15 10:27:34 PM: 	validation: 0.621760
12/15 10:27:34 PM: Statistic: micro_avg
12/15 10:27:34 PM: 	validation: 0.690718
12/15 10:27:34 PM: Statistic: mrpc_acc_f1
12/15 10:27:34 PM: 	training: 0.715858
12/15 10:27:34 PM: 	validation: 0.740588
12/15 10:27:34 PM: Statistic: mrpc_accuracy
12/15 10:27:34 PM: 	training: 0.655303
12/15 10:27:34 PM: 	validation: 0.691176
12/15 10:27:34 PM: Statistic: mrpc_f1
12/15 10:27:34 PM: 	training: 0.776413
12/15 10:27:34 PM: 	validation: 0.790000
12/15 10:27:34 PM: Statistic: mrpc_precision
12/15 10:27:34 PM: 	training: 0.683983
12/15 10:27:34 PM: 	validation: 0.738318
12/15 10:27:34 PM: Statistic: mrpc_recall
12/15 10:27:34 PM: 	training: 0.897727
12/15 10:27:34 PM: 	validation: 0.849462
12/15 10:27:34 PM: Statistic: sst_accuracy
12/15 10:27:34 PM: 	training: 0.696875
12/15 10:27:34 PM: 	validation: 0.688073
12/15 10:27:34 PM: Statistic: wnli_accuracy
12/15 10:27:34 PM: 	training: 0.436019
12/15 10:27:34 PM: 	validation: 0.436620
12/15 10:27:34 PM: global_lr: 0.000100
12/15 10:27:35 PM: Saved files to outputs/iwslt/iwslt
12/15 10:27:35 PM: Update 504: task mrpc, batch 1 (183): acc_f1: 0.8036, accuracy: 0.7500, f1: 0.8571, precision: 0.8571, recall: 0.8571, mrpc_loss: 0.5216 ||
12/15 10:27:37 PM: ***** Pass 600 / Epoch 6 *****
12/15 10:27:37 PM: mrpc: trained on 39 batches, 0.085 epochs
12/15 10:27:37 PM: sst: trained on 36 batches, 0.004 epochs
12/15 10:27:37 PM: wnli: trained on 25 batches, 0.312 epochs
12/15 10:27:37 PM: Validating...
12/15 10:27:38 PM: Best model found for sst.
12/15 10:27:38 PM: Best model found for micro.
12/15 10:27:38 PM: Advancing scheduler.
12/15 10:27:38 PM: 	Best macro_avg: 0.659
12/15 10:27:38 PM: 	# bad epochs: 0
12/15 10:27:38 PM: Statistic: mrpc_loss
12/15 10:27:38 PM: 	training: 0.634605
12/15 10:27:38 PM: 	validation: 0.590031
12/15 10:27:38 PM: Statistic: sst_loss
12/15 10:27:38 PM: 	training: 0.488873
12/15 10:27:38 PM: 	validation: 0.581074
12/15 10:27:38 PM: Statistic: wnli_loss
12/15 10:27:38 PM: 	training: 0.720842
12/15 10:27:38 PM: 	validation: 0.786205
12/15 10:27:38 PM: Statistic: macro_avg
12/15 10:27:38 PM: 	validation: 0.625825
12/15 10:27:38 PM: Statistic: micro_avg
12/15 10:27:38 PM: 	validation: 0.699521
12/15 10:27:38 PM: Statistic: mrpc_acc_f1
12/15 10:27:38 PM: 	training: 0.748130
12/15 10:27:38 PM: 	validation: 0.737874
12/15 10:27:38 PM: Statistic: mrpc_accuracy
12/15 10:27:38 PM: 	training: 0.689103
12/15 10:27:38 PM: 	validation: 0.686275
12/15 10:27:38 PM: Statistic: mrpc_f1
12/15 10:27:38 PM: 	training: 0.807157
12/15 10:27:38 PM: 	validation: 0.789474
12/15 10:27:38 PM: Statistic: mrpc_precision
12/15 10:27:38 PM: 	training: 0.700000
12/15 10:27:38 PM: 	validation: 0.729483
12/15 10:27:38 PM: Statistic: mrpc_recall
12/15 10:27:38 PM: 	training: 0.953052
12/15 10:27:38 PM: 	validation: 0.860215
12/15 10:27:38 PM: Statistic: sst_accuracy
12/15 10:27:38 PM: 	training: 0.777778
12/15 10:27:38 PM: 	validation: 0.702982
12/15 10:27:38 PM: Statistic: wnli_accuracy
12/15 10:27:38 PM: 	training: 0.485000
12/15 10:27:38 PM: 	validation: 0.436620
12/15 10:27:38 PM: global_lr: 0.000050
12/15 10:27:39 PM: Saved files to outputs/iwslt/iwslt
12/15 10:27:41 PM: ***** Pass 700 / Epoch 7 *****
12/15 10:27:41 PM: mrpc: trained on 32 batches, 0.070 epochs
12/15 10:27:41 PM: sst: trained on 38 batches, 0.005 epochs
12/15 10:27:41 PM: wnli: trained on 30 batches, 0.375 epochs
12/15 10:27:41 PM: Validating...
12/15 10:27:42 PM: Out of patience. Stopped tracking wnli
12/15 10:27:42 PM: Advancing scheduler.
12/15 10:27:42 PM: 	Best macro_avg: 0.659
12/15 10:27:42 PM: 	# bad epochs: 1
12/15 10:27:42 PM: Statistic: mrpc_loss
12/15 10:27:42 PM: 	training: 0.575819
12/15 10:27:42 PM: 	validation: 0.689818
12/15 10:27:42 PM: Statistic: sst_loss
12/15 10:27:42 PM: 	training: 0.525392
12/15 10:27:42 PM: 	validation: 0.613301
12/15 10:27:42 PM: Statistic: wnli_loss
12/15 10:27:42 PM: 	training: 0.715331
12/15 10:27:42 PM: 	validation: 0.737664
12/15 10:27:42 PM: Statistic: macro_avg
12/15 10:27:42 PM: 	validation: 0.610445
12/15 10:27:42 PM: Statistic: micro_avg
12/15 10:27:42 PM: 	validation: 0.689431
12/15 10:27:42 PM: Statistic: mrpc_acc_f1
12/15 10:27:42 PM: 	training: 0.782895
12/15 10:27:42 PM: 	validation: 0.753483
12/15 10:27:42 PM: Statistic: mrpc_accuracy
12/15 10:27:42 PM: 	training: 0.730469
12/15 10:27:42 PM: 	validation: 0.691176
12/15 10:27:42 PM: Statistic: mrpc_f1
12/15 10:27:42 PM: 	training: 0.835322
12/15 10:27:42 PM: 	validation: 0.815789
12/15 10:27:42 PM: Statistic: mrpc_precision
12/15 10:27:42 PM: 	training: 0.751073
12/15 10:27:42 PM: 	validation: 0.688889
12/15 10:27:42 PM: Statistic: mrpc_recall
12/15 10:27:42 PM: 	training: 0.940860
12/15 10:27:42 PM: 	validation: 1.000000
12/15 10:27:42 PM: Statistic: sst_accuracy
12/15 10:27:42 PM: 	training: 0.740132
12/15 10:27:42 PM: 	validation: 0.683486
12/15 10:27:42 PM: Statistic: wnli_accuracy
12/15 10:27:42 PM: 	training: 0.470833
12/15 10:27:42 PM: 	validation: 0.394366
12/15 10:27:42 PM: global_lr: 0.000050
12/15 10:27:43 PM: Saved files to outputs/iwslt/iwslt
12/15 10:27:44 PM: Update 760: task sst, batch 16 (244): accuracy: 0.7344, sst_loss: 0.4906 ||
12/15 10:27:44 PM: Update 790: task wnli, batch 38 (257): accuracy: 0.5217, wnli_loss: 0.7280 ||
12/15 10:27:44 PM: ***** Pass 800 / Epoch 8 *****
12/15 10:27:44 PM: mrpc: trained on 28 batches, 0.061 epochs
12/15 10:27:44 PM: sst: trained on 30 batches, 0.004 epochs
12/15 10:27:44 PM: wnli: trained on 42 batches, 0.525 epochs
12/15 10:27:44 PM: Validating...
12/15 10:27:46 PM: Best model found for mrpc.
12/15 10:27:46 PM: Best model found for sst.
12/15 10:27:46 PM: Best model found for micro.
12/15 10:27:46 PM: Advancing scheduler.
12/15 10:27:46 PM: 	Best macro_avg: 0.659
12/15 10:27:46 PM: 	# bad epochs: 0
12/15 10:27:46 PM: Statistic: mrpc_loss
12/15 10:27:46 PM: 	training: 0.684020
12/15 10:27:46 PM: 	validation: 0.586516
12/15 10:27:46 PM: Statistic: sst_loss
12/15 10:27:46 PM: 	training: 0.471612
12/15 10:27:46 PM: 	validation: 0.566998
12/15 10:27:46 PM: Statistic: wnli_loss
12/15 10:27:46 PM: 	training: 0.727689
12/15 10:27:46 PM: 	validation: 0.726922
12/15 10:27:46 PM: Statistic: macro_avg
12/15 10:27:46 PM: 	validation: 0.580834
12/15 10:27:46 PM: Statistic: micro_avg
12/15 10:27:46 PM: 	validation: 0.705645
12/15 10:27:46 PM: Statistic: mrpc_acc_f1
12/15 10:27:46 PM: 	training: 0.686751
12/15 10:27:46 PM: 	validation: 0.758154
12/15 10:27:46 PM: Statistic: mrpc_accuracy
12/15 10:27:46 PM: 	training: 0.625000
12/15 10:27:46 PM: 	validation: 0.698529
12/15 10:27:46 PM: Statistic: mrpc_f1
12/15 10:27:46 PM: 	training: 0.748503
12/15 10:27:46 PM: 	validation: 0.817778
12/15 10:27:46 PM: Statistic: mrpc_precision
12/15 10:27:46 PM: 	training: 0.661376
12/15 10:27:46 PM: 	validation: 0.696970
12/15 10:27:46 PM: Statistic: mrpc_recall
12/15 10:27:46 PM: 	training: 0.862069
12/15 10:27:46 PM: 	validation: 0.989247
12/15 10:27:46 PM: Statistic: sst_accuracy
12/15 10:27:46 PM: 	training: 0.766667
12/15 10:27:46 PM: 	validation: 0.716743
12/15 10:27:46 PM: Statistic: wnli_accuracy
12/15 10:27:46 PM: 	training: 0.516616
12/15 10:27:46 PM: 	validation: 0.267606
12/15 10:27:46 PM: global_lr: 0.000025
12/15 10:27:47 PM: Saved files to outputs/iwslt/iwslt
12/15 10:27:47 PM: Update 801: task mrpc, batch 1 (282): acc_f1: 0.4602, accuracy: 0.3750, f1: 0.5455, precision: 0.3750, recall: 1.0000, mrpc_loss: 0.8721 ||
12/15 10:27:48 PM: ***** Pass 900 / Epoch 9 *****
12/15 10:27:48 PM: mrpc: trained on 33 batches, 0.072 epochs
12/15 10:27:48 PM: sst: trained on 39 batches, 0.005 epochs
12/15 10:27:48 PM: wnli: trained on 28 batches, 0.350 epochs
12/15 10:27:48 PM: Validating...
12/15 10:27:50 PM: Best model found for micro.
12/15 10:27:50 PM: Best model found for macro.
12/15 10:27:50 PM: Advancing scheduler.
12/15 10:27:50 PM: 	Best macro_avg: 0.668
12/15 10:27:50 PM: 	# bad epochs: 0
12/15 10:27:50 PM: Statistic: mrpc_loss
12/15 10:27:50 PM: 	training: 0.635779
12/15 10:27:50 PM: 	validation: 0.575654
12/15 10:27:50 PM: Statistic: sst_loss
12/15 10:27:50 PM: 	training: 0.454632
12/15 10:27:50 PM: 	validation: 0.573880
12/15 10:27:50 PM: Statistic: wnli_loss
12/15 10:27:50 PM: 	training: 0.711314
12/15 10:27:50 PM: 	validation: 0.717310
12/15 10:27:50 PM: Statistic: macro_avg
12/15 10:27:50 PM: 	validation: 0.668354
12/15 10:27:50 PM: Statistic: micro_avg
12/15 10:27:50 PM: 	validation: 0.708763
12/15 10:27:50 PM: Statistic: mrpc_acc_f1
12/15 10:27:50 PM: 	training: 0.719576
12/15 10:27:50 PM: 	validation: 0.756224
12/15 10:27:50 PM: Statistic: mrpc_accuracy
12/15 10:27:50 PM: 	training: 0.655303
12/15 10:27:50 PM: 	validation: 0.698529
12/15 10:27:50 PM: Statistic: mrpc_f1
12/15 10:27:50 PM: 	training: 0.783848
12/15 10:27:50 PM: 	validation: 0.813918
12/15 10:27:50 PM: Statistic: mrpc_precision
12/15 10:27:50 PM: 	training: 0.662651
12/15 10:27:50 PM: 	validation: 0.704188
12/15 10:27:50 PM: Statistic: mrpc_recall
12/15 10:27:50 PM: 	training: 0.959302
12/15 10:27:50 PM: 	validation: 0.964158
12/15 10:27:50 PM: Statistic: sst_accuracy
12/15 10:27:50 PM: 	training: 0.778846
12/15 10:27:50 PM: 	validation: 0.699541
12/15 10:27:50 PM: Statistic: wnli_accuracy
12/15 10:27:50 PM: 	training: 0.455357
12/15 10:27:50 PM: 	validation: 0.549296
12/15 10:27:50 PM: global_lr: 0.000025
12/15 10:27:50 PM: Saved files to outputs/iwslt/iwslt
12/15 10:27:52 PM: ***** Pass 1000 / Epoch 10 *****
12/15 10:27:52 PM: mrpc: trained on 29 batches, 0.063 epochs
12/15 10:27:52 PM: sst: trained on 31 batches, 0.004 epochs
12/15 10:27:52 PM: wnli: trained on 40 batches, 0.500 epochs
12/15 10:27:52 PM: Validating...
12/15 10:27:54 PM: Advancing scheduler.
12/15 10:27:54 PM: 	Best macro_avg: 0.668
12/15 10:27:54 PM: 	# bad epochs: 1
12/15 10:27:54 PM: Maximum number of validations hit. Stopping training.
12/15 10:27:54 PM: Statistic: mrpc_loss
12/15 10:27:54 PM: 	training: 0.631062
12/15 10:27:54 PM: 	validation: 0.589893
12/15 10:27:54 PM: Statistic: sst_loss
12/15 10:27:54 PM: 	training: 0.522279
12/15 10:27:54 PM: 	validation: 0.572576
12/15 10:27:54 PM: Statistic: wnli_loss
12/15 10:27:54 PM: 	training: 0.696594
12/15 10:27:54 PM: 	validation: 0.744812
12/15 10:27:54 PM: Statistic: macro_avg
12/15 10:27:54 PM: 	validation: 0.620854
12/15 10:27:54 PM: Statistic: micro_avg
12/15 10:27:54 PM: 	validation: 0.700074
12/15 10:27:54 PM: Statistic: mrpc_acc_f1
12/15 10:27:54 PM: 	training: 0.713861
12/15 10:27:54 PM: 	validation: 0.756863
12/15 10:27:54 PM: Statistic: mrpc_accuracy
12/15 10:27:54 PM: 	training: 0.650862
12/15 10:27:54 PM: 	validation: 0.696078
12/15 10:27:54 PM: Statistic: mrpc_f1
12/15 10:27:54 PM: 	training: 0.776860
12/15 10:27:54 PM: 	validation: 0.817647
12/15 10:27:54 PM: Statistic: mrpc_precision
12/15 10:27:54 PM: 	training: 0.668246
12/15 10:27:54 PM: 	validation: 0.693267
12/15 10:27:54 PM: Statistic: mrpc_recall
12/15 10:27:54 PM: 	training: 0.927632
12/15 10:27:54 PM: 	validation: 0.996416
12/15 10:27:54 PM: Statistic: sst_accuracy
12/15 10:27:54 PM: 	training: 0.745968
12/15 10:27:54 PM: 	validation: 0.697248
12/15 10:27:54 PM: Statistic: wnli_accuracy
12/15 10:27:54 PM: 	training: 0.565079
12/15 10:27:54 PM: 	validation: 0.408451
12/15 10:27:54 PM: global_lr: 0.000025
12/15 10:27:54 PM: Saved files to outputs/iwslt/iwslt
12/15 10:27:54 PM: Stopped training after 10 validation checks
12/15 10:27:54 PM: Trained mrpc for 343 batches or 0.747 epochs
12/15 10:27:54 PM: Trained sst for 328 batches or 0.039 epochs
12/15 10:27:54 PM: Trained wnli for 329 batches or 4.112 epochs
12/15 10:27:54 PM: ***** VALIDATION RESULTS *****
12/15 10:27:54 PM: mrpc_acc_f1, 8, mrpc_loss: 0.58652, sst_loss: 0.56700, wnli_loss: 0.72692, macro_avg: 0.58083, micro_avg: 0.70565, mrpc_acc_f1: 0.75815, mrpc_accuracy: 0.69853, mrpc_f1: 0.81778, mrpc_precision: 0.69697, mrpc_recall: 0.98925, sst_accuracy: 0.71674, wnli_accuracy: 0.26761
12/15 10:27:54 PM: sst_accuracy, 8, mrpc_loss: 0.58652, sst_loss: 0.56700, wnli_loss: 0.72692, macro_avg: 0.58083, micro_avg: 0.70565, mrpc_acc_f1: 0.75815, mrpc_accuracy: 0.69853, mrpc_f1: 0.81778, mrpc_precision: 0.69697, mrpc_recall: 0.98925, sst_accuracy: 0.71674, wnli_accuracy: 0.26761
12/15 10:27:54 PM: wnli_accuracy, 1, mrpc_loss: 0.60417, sst_loss: 0.72567, wnli_loss: 0.70679, macro_avg: 0.60998, micro_avg: 0.58895, mrpc_acc_f1: 0.75166, mrpc_accuracy: 0.68873, mrpc_f1: 0.81460, mrpc_precision: 0.68719, mrpc_recall: 1.00000, sst_accuracy: 0.51491, wnli_accuracy: 0.56338
12/15 10:27:54 PM: micro_avg, 9, mrpc_loss: 0.57565, sst_loss: 0.57388, wnli_loss: 0.71731, macro_avg: 0.66835, micro_avg: 0.70876, mrpc_acc_f1: 0.75622, mrpc_accuracy: 0.69853, mrpc_f1: 0.81392, mrpc_precision: 0.70419, mrpc_recall: 0.96416, sst_accuracy: 0.69954, wnli_accuracy: 0.54930
12/15 10:27:54 PM: macro_avg, 9, mrpc_loss: 0.57565, sst_loss: 0.57388, wnli_loss: 0.71731, macro_avg: 0.66835, micro_avg: 0.70876, mrpc_acc_f1: 0.75622, mrpc_accuracy: 0.69853, mrpc_f1: 0.81392, mrpc_precision: 0.70419, mrpc_recall: 0.96416, sst_accuracy: 0.69954, wnli_accuracy: 0.54930
12/15 10:27:54 PM: Found no task-specific parameters to skip for task: iwslt
12/15 10:27:54 PM: Loaded model state from outputs/iwslt/iwslt/model_state_main_epoch_9.best_macro.th
12/15 10:27:54 PM: Fatal error in main():
Traceback (most recent call last):
  File "main.py", line 343, in <module>
    main(sys.argv[1:])
  File "main.py", line 288, in main
    pred_module = getattr(model, "%s_mdl" % task.name)
  File "/home/pm2758/anaconda3/envs/jiant/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __getattr__
    type(self).__name__, name))
AttributeError: 'MultiTaskModel' object has no attribute 'iwslt_mdl'
Epoch     6: reducing learning rate of group 0 to 5.0000e-05.
Epoch     8: reducing learning rate of group 0 to 2.5000e-05.
Traceback (most recent call last):
  File "main.py", line 353, in <module>
    raise e  # re-raise exception, in case debugger is attached.
  File "main.py", line 343, in <module>
    main(sys.argv[1:])
  File "main.py", line 288, in main
    pred_module = getattr(model, "%s_mdl" % task.name)
  File "/home/pm2758/anaconda3/envs/jiant/lib/python3.6/site-packages/torch/nn/modules/module.py", line 532, in __getattr__
    type(self).__name__, name))
AttributeError: 'MultiTaskModel' object has no attribute 'iwslt_mdl'
